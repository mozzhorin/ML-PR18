{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa8401d73c7a19e1a43fdd6a992ea9dcb60039a2"
   },
   "source": [
    "# Overview\n",
    "The notebook shows how to extract the segmentation map for the ships, augment the images and train a simple DNN model to detect them. A few additional tweaks like balancing the ship-count out a little better have been done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a6cd9d5ad61ffe3b8858769f20a5f9493f024a56"
   },
   "source": [
    "## Model Parameters\n",
    "We might want to adjust these later (or do some hyperparameter optimizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "301a5d939c566d1487a049bb2554d09b592b18b1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EDGE_CROP = 16\n",
    "GAUSSIAN_NOISE = 0.1\n",
    "UPSAMPLE_MODE = 'SIMPLE'\n",
    "# downsampling inside the network\n",
    "NET_SCALING = (1, 1)\n",
    "# downsampling in preprocessing\n",
    "IMG_SCALING = (2, 2)\n",
    "# number of validation images to use\n",
    "VALID_IMG_COUNT = 600\n",
    "# maximum number of steps_per_epoch in training\n",
    "MAX_TRAIN_STEPS = 150\n",
    "AUGMENT_BRIGHTNESS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util.montage import montage2d as montage\n",
    "montage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n",
    "ship_dir = '~/Downloads/airbus'\n",
    "train_image_dir = os.path.join(ship_dir, 'train')\n",
    "test_image_dir = os.path.join(ship_dir, 'test')\n",
    "import gc; gc.enable() # memory is tight\n",
    "\n",
    "from skimage.morphology import label\n",
    "\n",
    "def multi_rle_encode(img):\n",
    "    labels = label(img[:, :, 0])\n",
    "    return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n",
    "\n",
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def masks_as_image(in_mask_list):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "    #if isinstance(in_mask_list, list):\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks += rle_decode(mask)\n",
    "    return np.expand_dims(all_masks, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "3ca7119188fbb4c6540d9df55f5833b55435287e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131030 masks found\n",
      "104070\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00003e153.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000155de5.jpg</td>\n",
       "      <td>264661 17 265429 33 266197 33 266965 33 267733...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00021ddc3.jpg</td>\n",
       "      <td>101361 1 102128 3 102896 4 103663 6 104430 9 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00021ddc3.jpg</td>\n",
       "      <td>95225 2 95992 5 96760 7 97527 9 98294 9 99062 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00021ddc3.jpg</td>\n",
       "      <td>74444 4 75212 4 75980 4 76748 4 77517 3 78285 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00021ddc3.jpg</td>\n",
       "      <td>150423 2 151190 3 151958 3 152726 4 153495 3 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00021ddc3.jpg</td>\n",
       "      <td>139644 2 140408 6 141174 9 141942 9 142711 6 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00021ddc3.jpg</td>\n",
       "      <td>86727 2 87493 4 88261 4 89030 3 89798 4 90566 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00021ddc3.jpg</td>\n",
       "      <td>74441 3 75207 5 75975 5 76743 5 77511 5 78280 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00021ddc3.jpg</td>\n",
       "      <td>75972 3 76738 5 77506 5 78274 5 79042 6 79811 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId                                      EncodedPixels\n",
       "0  00003e153.jpg                                                NaN\n",
       "1  000155de5.jpg  264661 17 265429 33 266197 33 266965 33 267733...\n",
       "2  00021ddc3.jpg  101361 1 102128 3 102896 4 103663 6 104430 9 1...\n",
       "3  00021ddc3.jpg  95225 2 95992 5 96760 7 97527 9 98294 9 99062 ...\n",
       "4  00021ddc3.jpg  74444 4 75212 4 75980 4 76748 4 77517 3 78285 ...\n",
       "5  00021ddc3.jpg  150423 2 151190 3 151958 3 152726 4 153495 3 1...\n",
       "6  00021ddc3.jpg  139644 2 140408 6 141174 9 141942 9 142711 6 1...\n",
       "7  00021ddc3.jpg  86727 2 87493 4 88261 4 89030 3 89798 4 90566 ...\n",
       "8  00021ddc3.jpg  74441 3 75207 5 75975 5 76743 5 77511 5 78280 ...\n",
       "9  00021ddc3.jpg  75972 3 76738 5 77506 5 78274 5 79042 6 79811 ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = pd.read_csv(os.path.join(ship_dir, 'train_ship_segmentations.csv'))\n",
    "print(masks.shape[0], 'masks found')\n",
    "print(masks['ImageId'].value_counts().shape[0])\n",
    "masks.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fdedd5965f47f84aa8f3aab1cad978512781a1cc"
   },
   "source": [
    "# Make sure encode/decode works\n",
    "Given the process\n",
    "$$  RLE_0 \\stackrel{Decode}{\\longrightarrow} \\textrm{Image}_0 \\stackrel{Encode}{\\longrightarrow} RLE_1 \\stackrel{Decode}{\\longrightarrow} \\textrm{Image}_1 $$\n",
    "We want to check if/that\n",
    "$ \\textrm{Image}_0 \\stackrel{?}{=} \\textrm{Image}_1 $\n",
    "We could check the RLEs as well but that is more tedious. Also depending on how the objects have been labeled we might have different counts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "0081fd6f387abd7c05eb35f29575a2ee6ddc2236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Decoding->Encoding RLE_0: 9 -> RLE_1: 4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89b1bbdb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "rle_0 = masks.query('ImageId==\"00021ddc3.jpg\"')['EncodedPixels']\n",
    "img_0 = masks_as_image(rle_0)\n",
    "ax1.imshow(img_0[:, :, 0])\n",
    "ax1.set_title('Image$_0$')\n",
    "rle_1 = multi_rle_encode(img_0)\n",
    "img_1 = masks_as_image(rle_1)\n",
    "ax2.imshow(img_1[:, :, 0])\n",
    "ax2.set_title('Image$_1$')\n",
    "print('Check Decoding->Encoding',\n",
    "      'RLE_0:', len(rle_0), '->',\n",
    "      'RLE_1:', len(rle_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40cb72e241c0c3d8bc245b4e3c663b4a835b0011"
   },
   "source": [
    "# Split into training and validation groups\n",
    "We stratify by the number of boats appearing so we have nice balances in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4f008bf6898518fd371de013418f936edaa09f8",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\n",
    "unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "# some files are too small/corrupt\n",
    "unique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda c_img_id: \n",
    "                                                               os.stat(os.path.join(train_image_dir, \n",
    "                                                                                    c_img_id)).st_size/1024)\n",
    "unique_img_ids = unique_img_ids[unique_img_ids['file_size_kb']>50] # keep only 50kb files\n",
    "unique_img_ids['file_size_kb'].hist()\n",
    "masks.drop(['ships'], axis=1, inplace=True)\n",
    "unique_img_ids.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "871720221ac25f7f9408bfe01aeb4ccb95edbd1f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_ids, valid_ids = train_test_split(unique_img_ids, \n",
    "                 test_size = 0.3, \n",
    "                 stratify = unique_img_ids['ships'])\n",
    "train_df = pd.merge(masks, train_ids)\n",
    "valid_df = pd.merge(masks, valid_ids)\n",
    "print(train_df.shape[0], 'training masks')\n",
    "print(valid_df.shape[0], 'validation masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c21d5bff04bf9180463969ac120379345745ed03"
   },
   "source": [
    "### Examine Number of Ship Images\n",
    "Here we examine how often ships appear and replace the ones without any ships with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2612fa47c7e9fdcaa7aa720c4e15fc86fd65d69a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['ships'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ef8115a80749ac47f295e9a70217a5553970c2b3"
   },
   "source": [
    "# Undersample Empty Images\n",
    "Here we undersample the empty images to get a better balanced group with more ships to try and segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0cf0bb261eda957cb0a12a330260e1390c57c8c9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['grouped_ship_count'] = train_df['ships'].map(lambda x: (x+2)//3)\n",
    "balanced_train_df = train_df.groupby('grouped_ship_count').apply(lambda x: x.sample(1500))\n",
    "balanced_train_df['ships'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a3fb9fe33d81374c7bd836f5bc86a1df89190805"
   },
   "source": [
    "# Decode all the RLEs into Images\n",
    "We make a generator to produce batches of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6181ac51577e5636995e38a9e29311cf47f513ca",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_image_gen(in_df, batch_size = BATCH_SIZE):\n",
    "    all_batches = list(in_df.groupby('ImageId'))\n",
    "    out_rgb = []\n",
    "    out_mask = []\n",
    "    while True:\n",
    "        np.random.shuffle(all_batches)\n",
    "        for c_img_id, c_masks in all_batches:\n",
    "            rgb_path = os.path.join(train_image_dir, c_img_id)\n",
    "            c_img = imread(rgb_path)\n",
    "            c_mask = masks_as_image(c_masks['EncodedPixels'].values)\n",
    "            if IMG_SCALING is not None:\n",
    "                c_img = c_img[::IMG_SCALING[0], ::IMG_SCALING[1]]\n",
    "                c_mask = c_mask[::IMG_SCALING[0], ::IMG_SCALING[1]]\n",
    "            out_rgb += [c_img]\n",
    "            out_mask += [c_mask]\n",
    "            if len(out_rgb)>=batch_size:\n",
    "                yield np.stack(out_rgb, 0)/255.0, np.stack(out_mask, 0)\n",
    "                out_rgb, out_mask=[], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1983738da75b031f2bec8ba36db01c095e7c5d59",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_gen = make_image_gen(balanced_train_df)\n",
    "train_x, train_y = next(train_gen)\n",
    "print('x', train_x.shape, train_x.min(), train_x.max())\n",
    "print('y', train_y.shape, train_y.min(), train_y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b4396cd28ddd2e4c8076fcb165e9b61e3baeeeb7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (30, 10))\n",
    "batch_rgb = montage_rgb(train_x)\n",
    "batch_seg = montage(train_y[:, :, :, 0])\n",
    "ax1.imshow(batch_rgb)\n",
    "ax1.set_title('Images')\n",
    "ax2.imshow(batch_seg)\n",
    "ax2.set_title('Segmentations')\n",
    "ax3.imshow(mark_boundaries(batch_rgb, \n",
    "                           batch_seg.astype(int)))\n",
    "ax3.set_title('Outlined Ships')\n",
    "fig.savefig('overview.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f47639c987a10ebcb53e51f55aa8a11c98fa860"
   },
   "source": [
    "# Make the Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "30cb02a2a7103a9d66e90f701991199de1e5b73e",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_x, valid_y = next(make_image_gen(valid_df, VALID_IMG_COUNT))\n",
    "print(valid_x.shape, valid_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a8f65e7942816fb75b687a549dc1d5cc48d00e21"
   },
   "source": [
    "# Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "dg_args = dict(featurewise_center = False, \n",
    "                  samplewise_center = False,\n",
    "                  rotation_range = 15, \n",
    "                  width_shift_range = 0.1, \n",
    "                  height_shift_range = 0.1, \n",
    "                  shear_range = 0.01,\n",
    "                  zoom_range = [0.9, 1.25],  \n",
    "                  horizontal_flip = True, \n",
    "                  vertical_flip = True,\n",
    "                  fill_mode = 'reflect',\n",
    "                   data_format = 'channels_last')\n",
    "# brightness can be problematic since it seems to change the labels differently from the images \n",
    "if AUGMENT_BRIGHTNESS:\n",
    "    dg_args[' brightness_range'] = [0.5, 1.5]\n",
    "image_gen = ImageDataGenerator(**dg_args)\n",
    "\n",
    "if AUGMENT_BRIGHTNESS:\n",
    "    dg_args.pop('brightness_range')\n",
    "label_gen = ImageDataGenerator(**dg_args)\n",
    "\n",
    "def create_aug_gen(in_gen, seed = None):\n",
    "    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n",
    "    for in_x, in_y in in_gen:\n",
    "        seed = np.random.choice(range(9999))\n",
    "        # keep the seeds syncronized otherwise the augmentation to the images is different from the masks\n",
    "        g_x = image_gen.flow(255*in_x, \n",
    "                             batch_size = in_x.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "        g_y = label_gen.flow(in_y, \n",
    "                             batch_size = in_x.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "\n",
    "        yield next(g_x)/255.0, next(g_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6122ccb9e58bfac6fa5e11c86121e78d9e5151b1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_gen = create_aug_gen(train_gen)\n",
    "t_x, t_y = next(cur_gen)\n",
    "print('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\n",
    "print('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())\n",
    "# only keep first 9 samples to examine in detail\n",
    "t_x = t_x[:9]\n",
    "t_y = t_y[:9]\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\n",
    "ax1.imshow(montage_rgb(t_x), cmap='gray')\n",
    "ax1.set_title('images')\n",
    "ax2.imshow(montage(t_y[:, :, :, 0]), cmap='gray_r')\n",
    "ax2.set_title('ships')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "33300c4f03b6600da7b418f775d11d7ebf76a35a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ba08494eb9736ec3556b7c879143cdcdea89febf"
   },
   "source": [
    "# Build a Model\n",
    "Here we use a slight deviation on the U-Net standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2687377309d3cbbab1197f4eccd2b50ab996f5a6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "# Build U-Net model\n",
    "def upsample_conv(filters, kernel_size, strides, padding):\n",
    "    return layers.Conv2DTranspose(filters, kernel_size, strides=strides, padding=padding)\n",
    "def upsample_simple(filters, kernel_size, strides, padding):\n",
    "    return layers.UpSampling2D(strides)\n",
    "\n",
    "if UPSAMPLE_MODE=='DECONV':\n",
    "    upsample=upsample_conv\n",
    "else:\n",
    "    upsample=upsample_simple\n",
    "    \n",
    "input_img = layers.Input(t_x.shape[1:], name = 'RGB_Input')\n",
    "pp_in_layer = input_img\n",
    "if NET_SCALING is not None:\n",
    "    pp_in_layer = layers.AvgPool2D(NET_SCALING)(pp_in_layer)\n",
    "    \n",
    "pp_in_layer = layers.GaussianNoise(GAUSSIAN_NOISE)(pp_in_layer)\n",
    "pp_in_layer = layers.BatchNormalization()(pp_in_layer)\n",
    "\n",
    "c1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (pp_in_layer)\n",
    "c1 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\n",
    "p1 = layers.MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "c2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\n",
    "c2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\n",
    "p2 = layers.MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\n",
    "c3 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\n",
    "p3 = layers.MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n",
    "c4 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n",
    "p4 = layers.MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "\n",
    "c5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\n",
    "c5 = layers.Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n",
    "\n",
    "u6 = upsample(64, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "u6 = layers.concatenate([u6, c4])\n",
    "c6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\n",
    "c6 = layers.Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n",
    "\n",
    "u7 = upsample(32, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "u7 = layers.concatenate([u7, c3])\n",
    "c7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\n",
    "c7 = layers.Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n",
    "\n",
    "u8 = upsample(16, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "u8 = layers.concatenate([u8, c2])\n",
    "c8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\n",
    "c8 = layers.Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n",
    "\n",
    "u9 = upsample(8, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "u9 = layers.concatenate([u9, c1], axis=3)\n",
    "c9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\n",
    "c9 = layers.Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n",
    "\n",
    "d = layers.Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "d = layers.Cropping2D((EDGE_CROP, EDGE_CROP))(d)\n",
    "d = layers.ZeroPadding2D((EDGE_CROP, EDGE_CROP))(d)\n",
    "if NET_SCALING is not None:\n",
    "    d = layers.UpSampling2D(NET_SCALING)(d)\n",
    "\n",
    "seg_model = models.Model(inputs=[input_img], outputs=[d])\n",
    "seg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1678069aa8013510264ba898291c6ae2dce88a76",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
    "def dice_p_bce(in_gt, in_pred):\n",
    "    return 1e-3*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\n",
    "def true_positive_rate(y_true, y_pred):\n",
    "    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)\n",
    "seg_model.compile(optimizer=Adam(1e-4, decay=1e-6), loss=dice_p_bce, metrics=[dice_coef, 'binary_accuracy', true_positive_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7282d18de3aff1cee12ff89b7d511a391702814f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('seg_model')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, \n",
    "                                   patience=3, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001, cooldown=2, min_lr=1e-6)\n",
    "early = EarlyStopping(monitor=\"val_dice_coef\", \n",
    "                      mode=\"max\", \n",
    "                      patience=15) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b67d808c0b8c7e28bff41e6d3858ff6f09dd626",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "step_count = min(MAX_TRAIN_STEPS, balanced_train_df.shape[0]//BATCH_SIZE)\n",
    "aug_gen = create_aug_gen(make_image_gen(balanced_train_df))\n",
    "loss_history = [seg_model.fit_generator(aug_gen, \n",
    "                             steps_per_epoch=step_count, \n",
    "                             epochs=3, \n",
    "                             validation_data=(valid_x, valid_y),\n",
    "                             callbacks=callbacks_list,\n",
    "                            workers=1 # the generator is not very thread safe\n",
    "                                       )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a168c8b1af446b800f6129104906003ededd61c4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_loss(loss_history):\n",
    "    epich = np.cumsum(np.concatenate(\n",
    "        [np.linspace(0.5, 1, len(mh.epoch)) for mh in loss_history]))\n",
    "    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(22, 10))\n",
    "    _ = ax1.plot(epich,\n",
    "                 np.concatenate([mh.history['loss'] for mh in loss_history]),\n",
    "                 'b-',\n",
    "                 epich, np.concatenate(\n",
    "            [mh.history['val_loss'] for mh in loss_history]), 'r-')\n",
    "    ax1.legend(['Training', 'Validation'])\n",
    "    ax1.set_title('Loss')\n",
    "\n",
    "    _ = ax2.plot(epich, np.concatenate(\n",
    "        [mh.history['true_positive_rate'] for mh in loss_history]), 'b-',\n",
    "                     epich, np.concatenate(\n",
    "            [mh.history['val_true_positive_rate'] for mh in loss_history]),\n",
    "                     'r-')\n",
    "    ax2.legend(['Training', 'Validation'])\n",
    "    ax2.set_title('True Positive Rate\\n(Positive Accuracy)')\n",
    "    \n",
    "    _ = ax3.plot(epich, np.concatenate(\n",
    "        [mh.history['binary_accuracy'] for mh in loss_history]), 'b-',\n",
    "                     epich, np.concatenate(\n",
    "            [mh.history['val_binary_accuracy'] for mh in loss_history]),\n",
    "                     'r-')\n",
    "    ax3.legend(['Training', 'Validation'])\n",
    "    ax3.set_title('Binary Accuracy (%)')\n",
    "    \n",
    "    _ = ax4.plot(epich, np.concatenate(\n",
    "        [mh.history['dice_coef'] for mh in loss_history]), 'b-',\n",
    "                     epich, np.concatenate(\n",
    "            [mh.history['val_dice_coef'] for mh in loss_history]),\n",
    "                     'r-')\n",
    "    ax4.legend(['Training', 'Validation'])\n",
    "    ax4.set_title('DICE')\n",
    "\n",
    "show_loss(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce1167e9f09200f537e61f93f486168a13be1711",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg_model.load_weights(weight_path)\n",
    "seg_model.save('seg_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "275b411dc97a350aacaba46c8562efcf2658b1a7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_y = seg_model.predict(valid_x)\n",
    "print(pred_y.shape, pred_y.min(), pred_y.max(), pred_y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6a4fd2ca0cf47ba069a314356bf74c7b531c56ac",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 10))\n",
    "ax.hist(pred_y.ravel(), np.linspace(0, 1, 10))\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_yscale('log', nonposy='clip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0018ab172d18936f8cc2c5df33d2f840dc16bf4f"
   },
   "source": [
    "# Prepare Full Resolution Model\n",
    "Here we account for the scaling so everything can happen in the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "17408f0ee8dc16149b8eff0447a1427ab3ed82ba",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if IMG_SCALING is not None:\n",
    "    fullres_model = models.Sequential()\n",
    "    fullres_model.add(layers.AvgPool2D(IMG_SCALING, input_shape = (None, None, 3)))\n",
    "    fullres_model.add(seg_model)\n",
    "    fullres_model.add(layers.UpSampling2D(IMG_SCALING))\n",
    "else:\n",
    "    fullres_model = seg_model\n",
    "fullres_model.save('fullres_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "17edb177402ae51651692511827a7e9d60646533"
   },
   "source": [
    "# Run the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4911811f267f9f3397a58902da9e75c6f261ad40",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_paths = os.listdir(test_image_dir)\n",
    "print(len(test_paths), 'test images found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "73ef7b3b2a74bf64968c79b4005075d4f0e23143",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, m_axs = plt.subplots(20, 2, figsize = (10, 40))\n",
    "[c_ax.axis('off') for c_ax in m_axs.flatten()]\n",
    "for (ax1, ax2), c_img_name in zip(m_axs, test_paths):\n",
    "    c_path = os.path.join(test_image_dir, c_img_name)\n",
    "    c_img = imread(c_path)\n",
    "    first_img = np.expand_dims(c_img, 0)/255.0\n",
    "    first_seg = fullres_model.predict(first_img)\n",
    "    ax1.imshow(first_img[0])\n",
    "    ax1.set_title('Image')\n",
    "    ax2.imshow(first_seg[0, :, :, 0], vmin = 0, vmax = 1)\n",
    "    ax2.set_title('Prediction')\n",
    "fig.savefig('test_predictions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "11a6c6615131ff8c317f95a5097b46565ef21121",
    "collapsed": true
   },
   "source": [
    "# Submission\n",
    "Since gneerating the submission takes a long time and quite a bit of memory we run it in a seperate kernel located at https://www.kaggle.com/kmader/from-trained-u-net-to-submission-part-2 \n",
    "That kernel takes the model saved in this kernel and applies it to all the test data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
