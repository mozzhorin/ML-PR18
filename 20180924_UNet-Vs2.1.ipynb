{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "import os\n",
    "import struct\n",
    "import errno\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.utils import data as D\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import gc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "import os.path as osp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "# https://github.com/albu/albumentations\n",
    "from albumentations import (ToFloat, Resize,\n",
    "    CLAHE, RandomRotate90, Transpose, ShiftScaleRotate, Blur, OpticalDistortion, \n",
    "    GridDistortion, HueSaturationValue, IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, \n",
    "    MedianBlur, IAAPiecewiseAffine, IAASharpen, IAAEmboss, RandomContrast, RandomBrightness, \n",
    "    Flip, OneOf, Compose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "path = './airbus/'\n",
    "path_train = path + 'train/'\n",
    "path_test = path + 'test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbusDS_train(D.Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_train, aug, transform, ids, masks):\n",
    "        \"\"\" Intialize the dataset\n",
    "        \"\"\"\n",
    "        self.aug = aug\n",
    "        self.path_train = path_train\n",
    "        self.transform = transform\n",
    "        self.df = ids\n",
    "        self.masks = masks       \n",
    "        self.filenames = self.df['ImageId'].values\n",
    "        self.len = len(self.filenames)\n",
    "        \n",
    "    # You must override __getitem__ and __len__\n",
    "    def get_mask(self, ImageId):\n",
    "        img_masks = self.masks.loc[self.masks['ImageId'] == ImageId, 'EncodedPixels'].tolist()\n",
    "\n",
    "        # Take the individual ship masks and create a single mask array for all ships\n",
    "        all_masks = np.zeros((768, 768))\n",
    "        if img_masks == [-1]:\n",
    "            return all_masks\n",
    "        for mask in img_masks:\n",
    "            all_masks += rle_decode(mask)\n",
    "        return all_masks\n",
    "    \n",
    "    def get_label(self, ImageId):\n",
    "        '''Returns a label: 0 - no ship, 1 - has one or more ships.'''\n",
    "        label = int(self.df[self.df['ImageId']==ImageId]['has_ship'].values[0])\n",
    "        return label\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"       \n",
    "        \n",
    "        image = Image.open(str(self.path_train + self.filenames[index]))\n",
    "        ImageId = self.filenames[index]\n",
    "        label = self.get_label(ImageId)\n",
    "        mask = self.get_mask(ImageId)            \n",
    "        if self.aug:\n",
    "            data = {\"image\": np.array(image), \"mask\": mask}\n",
    "            transformed = self.transform(**data)\n",
    "            image = transformed['image']/255\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            return image, transformed['mask'][np.newaxis,:,:], label\n",
    "        else:\n",
    "        \n",
    "            return self.transform(image), mask[np.newaxis,:,:], label \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbusDS_val(AirbusDS_train):\n",
    "    \"\"\"\n",
    "    A customized data loader.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"       \n",
    "        \n",
    "        image = Image.open(str(self.path_train + self.filenames[index]))\n",
    "        ImageId = self.filenames[index]\n",
    "        label = self.get_label(ImageId)\n",
    "                    \n",
    "        if self.aug:\n",
    "            data = {\"image\": np.array(image)}\n",
    "            transformed = self.transform(**data)\n",
    "            image = transformed['image']/255\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            return image, label\n",
    "        else:\n",
    "            return self.transform(image), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbusDS:\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "        args (dict): Dictionary of (command line) arguments.\n",
    "            Needs to contain batch_size (int) and workers(int).\n",
    "        is_gpu (bool): True if CUDA is enabled.\n",
    "            Sets value of pin_memory in DataLoader.\n",
    "\n",
    "    Attributes:\n",
    "        trainset (torch.utils.data.TensorDataset): Training set wrapper.\n",
    "        valset (torch.utils.data.TensorDataset): Validation set wrapper.\n",
    "        train_loader (torch.utils.data.DataLoader): Training set loader with shuffling.\n",
    "        val_loader (torch.utils.data.DataLoader): Validation set loader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_gpu, batch_size, workers, root, aug=False, resize_factor=1, empty_frac=0.33, test_size=0.1):\n",
    "        \n",
    "        self.root = root\n",
    "        self.path_train = root + 'train/'\n",
    "        self.aug = aug\n",
    "        self.empty_frac = empty_frac\n",
    "        self.resize_factor = resize_factor\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        exclude_list = ['6384c3e78.jpg','13703f040.jpg', '14715c06d.jpg',  '33e0ff2d5.jpg',\n",
    "                '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg', \n",
    "                'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n",
    "                'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg'] # corrupted images   \n",
    "    \n",
    "        # Calculate masks\n",
    "        masks = pd.read_csv(str(self.root+'train_ship_segmentations.csv')).fillna(-1)\n",
    "        masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\n",
    "        \n",
    "        # Calculate the number of ships on the images\n",
    "        unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "        unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "        unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "        \n",
    "        # Drop corrupted images\n",
    "        unique_img_ids = unique_img_ids[~unique_img_ids['ImageId'].isin(exclude_list)]\n",
    "        self.images_df = unique_img_ids\n",
    "        \n",
    "        masks.drop(['ships'], axis=1, inplace=True)\n",
    "        self.masks = masks \n",
    "        \n",
    "        ##########################\n",
    "\n",
    "        self.trainset, self.valset = self.get_dataset()\n",
    "\n",
    "        self.train_loader, self.val_loader = self.get_dataset_loader (batch_size, workers, is_gpu)\n",
    "\n",
    "        self.val_loader.dataset.class_to_idx = {'no_ship': 0, 'ship': 1}\n",
    "\n",
    "\n",
    "    def get_dataset(self):\n",
    "        \"\"\"\n",
    "        Loads and wraps training and validation datasets\n",
    "\n",
    "        Returns:\n",
    "             torch.utils.data.TensorDataset: trainset, valset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Split dataset to train and validate sets to evaluate the model\n",
    "        train_ids, val_ids = train_test_split(self.images_df, test_size=self.test_size)\n",
    "        self.val_ids = val_ids\n",
    "        \n",
    "        # Drop small images (mostly just clouds, water or corrupted)\n",
    "        train_ids['file_size_kb'] = train_ids['ImageId'].map(lambda c_img_id: os.stat(os.path.join(self.path_train, c_img_id)).st_size/1024)\n",
    "        train_ids = train_ids[train_ids['file_size_kb']>40] # keep only >40kb files\n",
    "        \n",
    "        # Undersample empty images to balance the dataset\n",
    "        ships = train_ids[train_ids['has_ship']==1] \n",
    "        no_ships = train_ids[train_ids['has_ship']==0].sample(frac=self.empty_frac)  # Take only this fraction of empty images\n",
    "        self.train_ids = pd.concat([ships, no_ships], axis=0)        \n",
    "               \n",
    "        \n",
    "        # Define transformations for augmentation and without it\n",
    "        self.transform_no_aug = transforms.Compose([transforms.Resize((int(768/self.resize_factor),\n",
    "                                                                       int(768/self.resize_factor))), transforms.ToTensor()])\n",
    "        if self.aug:\n",
    "            self.transform = Compose([Resize(height=int(768/self.resize_factor), width=int(768/self.resize_factor)),\n",
    "                                      OneOf([RandomRotate90(), Transpose(), Flip()], p=0.3)])\n",
    "        else:\n",
    "            self.transform = self.transform_no_aug\n",
    "\n",
    "        # TensorDataset wrapper\n",
    "        trainset = AirbusDS_train(self.path_train, self.aug, self.transform, self.train_ids, self.masks)\n",
    "        valset = AirbusDS_val(self.path_train, False, self.transform_no_aug, self.val_ids, self.masks) \n",
    "\n",
    "        return trainset, valset\n",
    "\n",
    "    def get_dataset_loader(self, batch_size, workers, is_gpu):\n",
    "        \"\"\"\n",
    "        Defines the dataset loader for wrapped dataset\n",
    "\n",
    "        Parameters:\n",
    "            batch_size (int): Defines the batch size in data loader\n",
    "            workers (int): Number of parallel threads to be used by data loader\n",
    "            is_gpu (bool): True if CUDA is enabled so pin_memory is set to True\n",
    "\n",
    "        Returns:\n",
    "             torch.utils.data.TensorDataset: trainset, valset\n",
    "        \"\"\"\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=True,\n",
    "                                                   num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
    "        test_loader = torch.utils.data.DataLoader(self.valset, batch_size=batch_size, shuffle=True,\n",
    "                                                  num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
    "\n",
    "        return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnet import meter\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "is_gpu = torch.cuda.is_available()\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(outputs, targets):\n",
    "    \"\"\"\n",
    "    Evaluates a model's top k accuracy\n",
    "\n",
    "    Parameters:\n",
    "        outputs (torch.autograd.Variable): model output\n",
    "        targets (torch.autograd.Variable): ground-truths/labels\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        float: percentage of correct predictions\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    _, pred = torch.max(outputs.data, 1)\n",
    "    correct = (pred == targets).sum().item()\n",
    "\n",
    "    res = 100 * correct / batch_size\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, device, writer):\n",
    "    \"\"\"\n",
    "    Trains/updates the model for one epoch on the training dataset.\n",
    "\n",
    "    Parameters:\n",
    "        train_loader (torch.utils.data.DataLoader): The trainset dataloader\n",
    "        model (torch.nn.module): Model to be trained\n",
    "        criterion (torch.nn.criterion): Loss function\n",
    "        optimizer (torch.optim.optimizer): optimizer instance like SGD or Adam\n",
    "        device (string): cuda or cpu\n",
    "    \"\"\"\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, masks, targets = data\n",
    "        inputs = inputs.to(device).float()\n",
    "        targets = targets.to(device).long()\n",
    "        \n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1 = accuracy(outputs, targets)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        #top1.update(prec1, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 200 == 199:\n",
    "            writer.add_scalar('loss', losses.avg, i)\n",
    "            print('Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(loss=losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, device, writer, epoch=0):\n",
    "    \"\"\"\n",
    "    Evaluates/validates the model\n",
    "\n",
    "    Parameters:\n",
    "        val_loader (torch.utils.data.DataLoader): The validation or testset dataloader\n",
    "        model (torch.nn.module): Model to be evaluated/validated\n",
    "        criterion (torch.nn.criterion): Loss function\n",
    "        device (string): cuda or cpu\n",
    "    \"\"\"\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    confusion = meter.ConfusionMeter(len(val_loader.dataset.class_to_idx), normalized=False)\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device).float()\n",
    "        targets = targets.to(device).long()\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs, targets)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "\n",
    "        # add to confusion matrix\n",
    "        confusion.add(outputs.data, targets)\n",
    "\n",
    "    count = losses.count\n",
    "    writer.add_scalars('confusion matrix', {'00' : confusion.value()[0,0]/count, \n",
    "                                            '01' : confusion.value()[0,1]/count,\n",
    "                                            '10' : confusion.value()[1,0]/count,\n",
    "                                            '11' : confusion.value()[1,1]/count}, epoch)\n",
    "    writer.add_scalar('Accuracy', top1.avg, epoch)\n",
    "    print(' * Validation accuracy: Prec@1 {top1.avg:.3f} '.format(top1=top1))\n",
    "    print('Confusion matrix: ', confusion.value()/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation from https://github.com/timctho/unet-pytorch/\n",
    "class UNet_down_block(torch.nn.Module):\n",
    "    def __init__(self, input_channel, output_channel, down_size):\n",
    "        super(UNet_down_block, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(input_channel, output_channel, 3, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(output_channel)\n",
    "        self.conv2 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(output_channel)\n",
    "        self.conv3 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(output_channel)\n",
    "        self.max_pool = torch.nn.MaxPool2d(2, 2)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.down_size = down_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.down_size:\n",
    "            x = self.max_pool(x)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        return x\n",
    "\n",
    "class UNet_up_block(torch.nn.Module):\n",
    "    def __init__(self, prev_channel, input_channel, output_channel):\n",
    "        super(UNet_up_block, self).__init__()\n",
    "        self.up_sampling = torch.nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        self.conv1 = torch.nn.Conv2d(prev_channel + input_channel, output_channel, 3, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(output_channel)\n",
    "        self.conv2 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(output_channel)\n",
    "        self.conv3 = torch.nn.Conv2d(output_channel, output_channel, 3, padding=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(output_channel)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, prev_feature_map, x):\n",
    "        x = self.up_sampling(x)\n",
    "        x = torch.cat((x, prev_feature_map), dim=1)\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.down_block1 = UNet_down_block(3, 16, False)\n",
    "        self.down_block2 = UNet_down_block(16, 32, True)\n",
    "        self.down_block3 = UNet_down_block(32, 64, True)\n",
    "        self.down_block4 = UNet_down_block(64, 128, True)\n",
    "        self.down_block5 = UNet_down_block(128, 256, True)\n",
    "        self.down_block6 = UNet_down_block(256, 512, True)\n",
    "        self.down_block7 = UNet_down_block(512, 1024, True)\n",
    "\n",
    "        self.mid_conv1 = torch.nn.Conv2d(1024, 1024, 3, padding=1)\n",
    "        self.bn1 = torch.nn.BatchNorm2d(1024)\n",
    "        self.mid_conv2 = torch.nn.Conv2d(1024, 1024, 3, padding=1)\n",
    "        self.bn2 = torch.nn.BatchNorm2d(1024)\n",
    "        self.mid_conv3 = torch.nn.Conv2d(1024, 1024, 3, padding=1)\n",
    "        self.bn3 = torch.nn.BatchNorm2d(1024)\n",
    "\n",
    "        self.up_block1 = UNet_up_block(512, 1024, 512)\n",
    "        self.up_block2 = UNet_up_block(256, 512, 256)\n",
    "        self.up_block3 = UNet_up_block(128, 256, 128)\n",
    "        self.up_block4 = UNet_up_block(64, 128, 64)\n",
    "        self.up_block5 = UNet_up_block(32, 64, 32)\n",
    "        self.up_block6 = UNet_up_block(16, 32, 16)\n",
    "\n",
    "        self.last_conv1 = torch.nn.Conv2d(16, 16, 3, padding=1)\n",
    "        self.last_bn = torch.nn.BatchNorm2d(16)\n",
    "        self.last_conv2 = torch.nn.Conv2d(16, 1, 1, padding=0)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x1 = self.down_block1(x)\n",
    "        self.x2 = self.down_block2(self.x1)\n",
    "        self.x3 = self.down_block3(self.x2)\n",
    "        self.x4 = self.down_block4(self.x3)\n",
    "        self.x5 = self.down_block5(self.x4)\n",
    "        self.x6 = self.down_block6(self.x5)\n",
    "        self.x7 = self.down_block7(self.x6)\n",
    "        self.x7 = self.relu(self.bn1(self.mid_conv1(self.x7)))\n",
    "        self.x7 = self.relu(self.bn2(self.mid_conv2(self.x7)))\n",
    "        self.x7 = self.relu(self.bn3(self.mid_conv3(self.x7)))\n",
    "        x = self.up_block1(self.x6, self.x7)\n",
    "        x = self.up_block2(self.x5, x)\n",
    "        x = self.up_block3(self.x4, x)\n",
    "        x = self.up_block4(self.x3, x)\n",
    "        x = self.up_block5(self.x2, x)\n",
    "        x = self.up_block6(self.x1, x)\n",
    "        x = self.relu(self.last_bn(self.last_conv1(x)))\n",
    "        x = self.last_conv2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "is_gpu = torch.cuda.is_available()\n",
    "batch_size = 1\n",
    "workers = 0\n",
    "path = './airbus/'\n",
    "aug=True\n",
    "resize_factor=4\n",
    "empty_frac=1\n",
    "test_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AirbusDS(torch.cuda.is_available(), batch_size, workers, path, aug, resize_factor, empty_frac, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function (criterion)\n",
    "model = UNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# we can use advanced stochastic gradient descent algorithms \n",
    "# with regularization (weight-decay) or momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 2018-09-24 18:32:38\n",
      "TRAIN\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1 at c:\\users\\administrator\\downloads\\new-builder\\win-wheel\\pytorch\\aten\\src\\thnn\\generic/SpatialClassNLLCriterion.c:60",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-0ab5620dbc2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"EPOCH:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y-%m-%d %H:%M:%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TRAIN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"VALIDATION\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%Y-%m-%d %H:%M:%S\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-47370e67e882>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, optimizer, device, writer)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# compute output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# measure accuracy and record loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[1;32m--> 759\u001b[1;33m                                self.ignore_index, self.reduce)\n\u001b[0m\u001b[0;32m    760\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m     \"\"\"\n\u001b[1;32m-> 1442\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1443\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: invalid argument 3: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1 at c:\\users\\administrator\\downloads\\new-builder\\win-wheel\\pytorch\\aten\\src\\thnn\\generic/SpatialClassNLLCriterion.c:60"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter('runs/U_Net')\n",
    "\n",
    "total_epochs = 2\n",
    "for epoch in range(total_epochs):\n",
    "    print(\"EPOCH:\", epoch + 1, time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    print(\"TRAIN\")\n",
    "    train(dataset.train_loader, model, criterion, optimizer, device, writer)\n",
    "    print(\"VALIDATION\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    validate(dataset.val_loader, model, criterion, device, writer, epoch)\n",
    "    torch.save(model, 'unet.model')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
