{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting classes for the new test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from airbus_dataloader import *\n",
    "from airbus_train_val_functions import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*45*45, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*45*45)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CNN_32_x4(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_32_x4, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 5) # input features, output features, kernel size\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.mp1 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) # input features, output features, kernel size\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.mp2 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
    "        \n",
    "        self.fc = nn.Linear(64*45*45, num_classes) # 4x4 is the remaining spatial resolution here\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mp1(self.act1(self.conv1(x)))\n",
    "        x = self.mp2(self.act2(self.conv2(x)))\n",
    "        # The view flattens the output to a vector (the representation needed by the classifier)\n",
    "        x = x.view(-1, 64*45*45)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class CNN_8_x4(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_8_x4, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 8, 5) # input features, output features, kernel size\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.mp1 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(8, 16, 5) # input features, output features, kernel size\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.mp2 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
    "        \n",
    "        self.fc = nn.Linear(16*45*45, num_classes) # 4x4 is the remaining spatial resolution here\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mp1(self.act1(self.conv1(x)))\n",
    "        x = self.mp2(self.act2(self.conv2(x)))\n",
    "        # The view flattens the output to a vector (the representation needed by the classifier)\n",
    "        x = x.view(-1, 16*45*45)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "is_gpu = torch.cuda.is_available()\n",
    "batch_size = 8\n",
    "workers = 4\n",
    "path = '../../airbus/'\n",
    "aug=True\n",
    "resize_factor=4\n",
    "empty_frac=1\n",
    "test_size=0.1\n",
    "    \n",
    "dataset = AirbusDS(torch.cuda.is_available(), batch_size, workers, \n",
    "                   path, aug, resize_factor, empty_frac, test_size)\n",
    "\n",
    "#model = CNN_8_x4(2).to(device)\n",
    "model = torch.load('CNN_8_x4_bal.model', map_location=device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START PREDICTIONS 2018-10-13 11:05:27\n",
      "Predicted 0 from 1951\t\n",
      "Predicted 1000 from 1951\t\n",
      "FINISH PREDICTIONS 2018-10-13 11:06:53\n",
      "PREDICTIONS RUN TIME (s) 86.84315371513367\n"
     ]
    }
   ],
   "source": [
    "predict(dataset.test_loader, model, device, predict_file='predictions05_v2.txt', threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred05 = pd.read_csv('predictions05_v2.txt', delimiter='\\t', names=['ImageId', 'Label']).fillna(-1)\n",
    "pred04 = pd.read_csv('predictions04_v2.txt', delimiter='\\t', names=['ImageId', 'Label']).fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new test set has 15606 images.\n"
     ]
    }
   ],
   "source": [
    "test_set_size = pred05['Label'].count()\n",
    "print('The new test set has {r} images.'.format(r=test_set_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject05 = pred05[pred05['Label'] == 0].count().values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We reject 10192 images or 65.31%\n",
      "We left 5414 images or 34.69%\n"
     ]
    }
   ],
   "source": [
    "print('We reject {r} images or {d:.2f}%'.format(r=reject05, d=reject05/test_set_size*100))\n",
    "print('We left {r} images or {d:.2f}%'.format(r=test_set_size-reject05, \n",
    "                                              d=(test_set_size-reject05)/test_set_size*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reject04 = pred04[pred04['Label'] == 0].count().values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We reject 5439 images or 34.85%\n",
      "We left 10167 images or 65.15%\n"
     ]
    }
   ],
   "source": [
    "print('We reject {r} images or {d:.2f}%'.format(r=reject04, d=reject04/test_set_size*100))\n",
    "print('We left {r} images or {d:.2f}%'.format(r=test_set_size-reject04, \n",
    "                                              d=(test_set_size-reject04)/test_set_size*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
