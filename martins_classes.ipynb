{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "import struct\n",
    "import gzip\n",
    "import errno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data as D\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchnet import meter\n",
    "import time\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "# https://github.com/albu/albumentations\n",
    "from albumentations import (ToFloat, Resize,\n",
    "    CLAHE, RandomRotate90, Transpose, ShiftScaleRotate, Blur, OpticalDistortion, \n",
    "    GridDistortion, HueSaturationValue, IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, \n",
    "    MedianBlur, IAAPiecewiseAffine, IAASharpen, IAAEmboss, RandomContrast, RandomBrightness, \n",
    "    Flip, OneOf, Compose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbusDS_train(D.Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_train, aug, transform, ids, masks):\n",
    "        \"\"\" Intialize the dataset\n",
    "        \"\"\"\n",
    "        self.aug = aug\n",
    "        self.path_train = path_train\n",
    "        self.transform = transform\n",
    "        self.df = ids\n",
    "        self.masks = masks       \n",
    "        self.filenames = self.df['ImageId'].values\n",
    "        self.len = len(self.filenames)\n",
    "        \n",
    "    # You must override __getitem__ and __len__\n",
    "    def get_mask(self, ImageId):\n",
    "        img_masks = self.masks.loc[self.masks['ImageId'] == ImageId, 'EncodedPixels'].tolist()\n",
    "\n",
    "        # Take the individual ship masks and create a single mask array for all ships\n",
    "        all_masks = np.zeros((768, 768))\n",
    "        if img_masks == [-1]:\n",
    "            return all_masks\n",
    "        for mask in img_masks:\n",
    "            all_masks += rle_decode(mask)\n",
    "        return all_masks\n",
    "    \n",
    "    def get_label(self, ImageId):\n",
    "        '''Returns a label: 0 - no ship, 1 - has one or more ships.'''\n",
    "        label = int(self.df[self.df['ImageId']==ImageId]['has_ship'].values[0])\n",
    "        return label\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"       \n",
    "        \n",
    "        image = Image.open(str(self.path_train + self.filenames[index]))\n",
    "        ImageId = self.filenames[index]\n",
    "        label = self.get_label(ImageId)\n",
    "        mask = self.get_mask(ImageId)            \n",
    "        if self.aug:\n",
    "            data = {\"image\": np.array(image), \"mask\": mask}\n",
    "            transformed = self.transform(**data)\n",
    "            image = transformed['image']/255\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            return image, transformed['mask'][np.newaxis,:,:], label\n",
    "        else:\n",
    "        \n",
    "            return self.transform(image), mask[np.newaxis,:,:], label \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "class AirbusDS_val(AirbusDS_train):\n",
    "    \"\"\"\n",
    "    A customized data loader.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"       \n",
    "        \n",
    "        image = Image.open(str(self.path_train + self.filenames[index]))\n",
    "        ImageId = self.filenames[index]\n",
    "        label = self.get_label(ImageId)\n",
    "                    \n",
    "        if self.aug:\n",
    "            data = {\"image\": np.array(image)}\n",
    "            transformed = self.transform(**data)\n",
    "            image = transformed['image']/255\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            return image, label\n",
    "        else:\n",
    "            return self.transform(image), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbusDS:\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "        args (dict): Dictionary of (command line) arguments.\n",
    "            Needs to contain batch_size (int) and workers(int).\n",
    "        is_gpu (bool): True if CUDA is enabled.\n",
    "            Sets value of pin_memory in DataLoader.\n",
    "\n",
    "    Attributes:\n",
    "        trainset (torch.utils.data.TensorDataset): Training set wrapper.\n",
    "        valset (torch.utils.data.TensorDataset): Validation set wrapper.\n",
    "        train_loader (torch.utils.data.DataLoader): Training set loader with shuffling.\n",
    "        val_loader (torch.utils.data.DataLoader): Validation set loader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_gpu, batch_size, workers, root, \\\n",
    "                 aug=False, resize_factor=1, empty_frac=0.33, test_size=0.1):\n",
    "        \n",
    "        self.root = root\n",
    "        self.path_train = root + 'train/'\n",
    "        self.aug = aug\n",
    "        self.empty_frac = empty_frac\n",
    "        self.resize_factor = resize_factor\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        exclude_list = ['6384c3e78.jpg','13703f040.jpg', '14715c06d.jpg',  '33e0ff2d5.jpg',\n",
    "                '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg', \n",
    "                'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n",
    "                'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg'] # corrupted images   \n",
    "    \n",
    "        # Calculate masks\n",
    "        masks = pd.read_csv(str(self.root+'train_ship_segmentations.csv')).fillna(-1)\n",
    "        masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\n",
    "        \n",
    "        # Calculate the number of ships on the images\n",
    "        unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "        unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "        unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "        \n",
    "        # Drop corrupted images\n",
    "        unique_img_ids = unique_img_ids[~unique_img_ids['ImageId'].isin(exclude_list)]\n",
    "        self.images_df = unique_img_ids\n",
    "        \n",
    "        masks.drop(['ships'], axis=1, inplace=True)\n",
    "        self.masks = masks \n",
    "        \n",
    "        ##########################\n",
    "\n",
    "        self.trainset, self.valset = self.get_dataset()\n",
    "\n",
    "        self.train_loader, self.val_loader = self.get_dataset_loader (batch_size, workers, is_gpu)\n",
    "\n",
    "        self.val_loader.dataset.class_to_idx = {'no_ship': 0, 'ship': 1}\n",
    "\n",
    "\n",
    "    def get_dataset(self):\n",
    "        \"\"\"\n",
    "        Loads and wraps training and validation datasets\n",
    "\n",
    "        Returns:\n",
    "             torch.utils.data.TensorDataset: trainset, valset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Split dataset to train and validate sets to evaluate the model\n",
    "        train_ids, val_ids = train_test_split(self.images_df, test_size=test_size)\n",
    "        self.val_ids = val_ids\n",
    "        \n",
    "        # Drop small images (mostly just clouds, water or corrupted)\n",
    "        train_ids['file_size_kb'] = train_ids['ImageId'].map(\\\n",
    "                            lambda c_img_id: os.stat(os.path.join(self.path_train, c_img_id)).st_size/1024)\n",
    "        train_ids = train_ids[train_ids['file_size_kb']>40] # keep only >40kb files\n",
    "        \n",
    "        # Undersample empty images to balance the dataset\n",
    "        ships = train_ids[train_ids['has_ship']==1] \n",
    "        no_ships = train_ids[train_ids['has_ship']==0].sample(frac=self.empty_frac)  # Take only this fraction of empty images\n",
    "        self.train_ids = pd.concat([ships, no_ships], axis=0)        \n",
    "               \n",
    "        \n",
    "        # Define transformations for augmentation and without it\n",
    "        self.transform_no_aug = transforms.Compose([transforms.Resize((int(768/resize_factor), int(768/resize_factor))),\n",
    "                                                 transforms.ToTensor()])\n",
    "        if self.aug:\n",
    "            self.transform = Compose([Resize(height=int(768/resize_factor), width=int(768/resize_factor)),\n",
    "                                      OneOf([RandomRotate90(), Transpose(), Flip()], p=0.3)])\n",
    "        else:\n",
    "            self.transform = self.transform_no_aug\n",
    "\n",
    "        # TensorDataset wrapper\n",
    "        trainset = AirbusDS_train(self.path_train, self.aug, self.transform, self.train_ids, self.masks)\n",
    "        valset = AirbusDS_val(self.path_train, False, self.transform_no_aug, self.val_ids, self.masks) \n",
    "\n",
    "        return trainset, valset\n",
    "\n",
    "    def get_dataset_loader(self, batch_size, workers, is_gpu):\n",
    "        \"\"\"\n",
    "        Defines the dataset loader for wrapped dataset\n",
    "\n",
    "        Parameters:\n",
    "            batch_size (int): Defines the batch size in data loader\n",
    "            workers (int): Number of parallel threads to be used by data loader\n",
    "            is_gpu (bool): True if CUDA is enabled so pin_memory is set to True\n",
    "\n",
    "        Returns:\n",
    "             torch.utils.data.TensorDataset: trainset, valset\n",
    "        \"\"\"\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=True,\n",
    "                                                   num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
    "        test_loader = torch.utils.data.DataLoader(self.valset, batch_size=batch_size, shuffle=True,\n",
    "                                                  num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
    "\n",
    "        return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "is_gpu = torch.cuda.is_available()\n",
    "batch_size = 2\n",
    "workers = 4\n",
    "path = '../airbus/'\n",
    "aug=True\n",
    "resize_factor=4\n",
    "empty_frac=1\n",
    "test_size=0.1\n",
    "    \n",
    "dataset = AirbusDS(torch.cuda.is_available(), batch_size, workers, \\\n",
    "                   path, aug, resize_factor, empty_frac, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*45*45, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*45*45)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_fashion(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_fashion, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 5) # input features, output features, kernel size\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.mp1 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) # input features, output features, kernel size\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.mp2 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
    "        \n",
    "        self.fc = nn.Linear(64*45*45, num_classes) # 4x4 is the remaining spatial resolution here\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mp1(self.act1(self.conv1(x)))\n",
    "        x = self.mp2(self.act2(self.conv2(x)))\n",
    "        # The view flattens the output to a vector (the representation needed by the classifier)\n",
    "        x = x.view(-1, 64*45*45)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining optimization criterion and optimizer\n",
    "A good baseline is a Cross Entropy loss (Log Softmax + negative log-likelihood) and a stochastic gradient descent (SGD) algorithm with a baseline learning rate of 0.01. If we want to we can use additional momenta or regularization terms (such as L2 - Tikhonov regularization commonly reffered to as weight-decay in ML). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function (criterion)\n",
    "model = CNN_fashion(2).to(device)\n",
    "#model = torch.load('CNN_fashion.model', map_location='cpu').to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# we can use advanced stochastic gradient descent algorithms \n",
    "# with regularization (weight-decay) or momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=5e-4)\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring and calculating accuracy\n",
    "We add a convenience class to keep track and average concepts such as processing or data loading speeds, losses and accuracies. For this we need to define a function to define accuracy, which could be based on the absolute accuracy, or top-1 accuracy. Often times in Machine Learning other metrics are employed. For example, in the ImageNet ILSVRC challenge with a classification problem containing a 1000 classes, it is common to report the top-5 accuracy. Here a prediction is counted as accurate if the correct class lies within the top-5 most likely output classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(outputs, targets):\n",
    "    \"\"\"\n",
    "    Evaluates a model's top k accuracy\n",
    "\n",
    "    Parameters:\n",
    "        outputs (torch.autograd.Variable): model output\n",
    "        targets (torch.autograd.Variable): ground-truths/labels\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        float: percentage of correct predictions\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    _, pred = torch.max(outputs.data, 1)\n",
    "    correct = (pred == targets).sum().item()\n",
    "\n",
    "    res = 100 * correct / batch_size\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function (sometimes referred to as \"hook\")\n",
    "The training function needs to loop through the entire dataset in steps of mini-batches (for SGD). For each mini-batch the output of the model and losses are calculated and a \"backward\" pass is done in order to do an update to the model's weights. When the entire dataset has been processed once, one epoch of the training has been conducted. It is common to shuffle the dataset after each epoch. In this implementation this is handled by the \"sampler\" of the dataset loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, device, writer):\n",
    "    \"\"\"\n",
    "    Trains/updates the model for one epoch on the training dataset.\n",
    "\n",
    "    Parameters:\n",
    "        train_loader (torch.utils.data.DataLoader): The trainset dataloader\n",
    "        model (torch.nn.module): Model to be trained\n",
    "        criterion (torch.nn.criterion): Loss function\n",
    "        optimizer (torch.optim.optimizer): optimizer instance like SGD or Adam\n",
    "        device (string): cuda or cpu\n",
    "    \"\"\"\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, masks, targets = data\n",
    "        inputs = inputs.to(device).float()\n",
    "        targets = targets.to(device).long()\n",
    "        \n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1 = accuracy(outputs, targets)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        #top1.update(prec1, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 200 == 199:\n",
    "            writer.add_scalar('loss', losses.avg, i)\n",
    "            print('Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(loss=losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation function\n",
    "Validation is similar to the training loop, but on a separate dataset with the exception that no update to the weights is performed. This way we can monitor the generalization ability of our model and check whether it is overfitting (memorizing) the training dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, device, writer, epoch=0):\n",
    "    \"\"\"\n",
    "    Evaluates/validates the model\n",
    "\n",
    "    Parameters:\n",
    "        val_loader (torch.utils.data.DataLoader): The validation or testset dataloader\n",
    "        model (torch.nn.module): Model to be evaluated/validated\n",
    "        criterion (torch.nn.criterion): Loss function\n",
    "        device (string): cuda or cpu\n",
    "    \"\"\"\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    confusion = meter.ConfusionMeter(len(val_loader.dataset.class_to_idx), normalized=False)\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device).float()\n",
    "        targets = targets.to(device).long()\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs, targets)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "\n",
    "        # add to confusion matrix\n",
    "        confusion.add(outputs.data, targets)\n",
    "\n",
    "    count = losses.count\n",
    "    writer.add_scalars('confusion matrix', {'00' : confusion.value()[0,0]/count, \n",
    "                                            '01' : confusion.value()[0,1]/count,\n",
    "                                            '10' : confusion.value()[1,0]/count,\n",
    "                                            '11' : confusion.value()[1,1]/count}, epoch)\n",
    "    writer.add_scalar('Accuracy', top1.avg, epoch)\n",
    "    print(' * Validation accuracy: Prec@1 {top1.avg:.3f} '.format(top1=top1))\n",
    "    print('Confusion matrix: ', confusion.value()/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1 2018-09-13 19:32:54\n",
      "TRAIN\n",
      "Loss 0.6930 (0.6930)\t\n",
      "Loss 1.1115 (0.6059)\t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Process Process-3:\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "Process Process-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-4b1825255699>\", line 39, in __getitem__\n",
      "    label = self.get_label(ImageId)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 763, in _comp_method_OBJECT_ARRAY\n",
      "    result = lib.scalar_compare(x, y, op)\n",
      "  File \"<ipython-input-4-4b1825255699>\", line 30, in get_label\n",
      "    label = int(self.df[self.df['ImageId']==ImageId]['has_ship'].values[0])\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 879, in wrapper\n",
      "    res = na_op(values, other)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\", line 783, in na_op\n",
      "    result = _comp_method_OBJECT_ARRAY(op, x, y)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/mo/anaconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5d5ca3a91058>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EPOCH:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d %H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRAIN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"VALIDATION\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y-%m-%d %H:%M:%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-f2c25a9fbdaa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, criterion, optimizer, device, writer)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-e297da8866b0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmp1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmp2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m# The view flattens the output to a vector (the representation needed by the classifier)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_epochs = 10\n",
    "for epoch in range(total_epochs):\n",
    "    print(\"EPOCH:\", epoch + 1, time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    print(\"TRAIN\")\n",
    "    train(dataset.train_loader, model, criterion, optimizer, device, writer)\n",
    "    print(\"VALIDATION\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    validate(dataset.val_loader, model, criterion, device, writer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confus = np.array([[5815, 1748],[ 515, 2329]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confus.T/len(dataset.valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#torch.save(model, '../CNN_fashion.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "dataiter = iter(dataset.val_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.to(device).float()\n",
    "labels = labels.to(device).long()\n",
    "outputs = model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(outputs.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(images[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Validation accuracy: Prec@1 52.695 \n",
      "Confusion matrix:  [[0.42538676 0.29508984]\n",
      " [0.17795714 0.10156625]]\n"
     ]
    }
   ],
   "source": [
    "validate(dataset.val_loader, model, criterion, device, writer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
