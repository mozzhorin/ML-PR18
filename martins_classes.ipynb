{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "import struct\n",
    "import gzip\n",
    "import errno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils import data as D\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchnet import meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# <api>\n",
    "# https://github.com/albu/albumentations\n",
    "from albumentations import (ToFloat, Resize,\n",
    "    CLAHE, RandomRotate90, Transpose, ShiftScaleRotate, Blur, OpticalDistortion, \n",
    "    GridDistortion, HueSaturationValue, IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, \n",
    "    MedianBlur, IAAPiecewiseAffine, IAASharpen, IAAEmboss, RandomContrast, RandomBrightness, \n",
    "    Flip, OneOf, Compose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbusDS_train(D.Dataset):\n",
    "    \"\"\"\n",
    "    A customized data loader.\n",
    "    \"\"\"\n",
    "    def __init__(self, path_train, aug, transform, ids, masks):\n",
    "        \"\"\" Intialize the dataset\n",
    "        \"\"\"\n",
    "        self.aug = aug\n",
    "        self.path_train = path_train\n",
    "        self.transform = transform\n",
    "        self.df = ids\n",
    "        self.masks = masks       \n",
    "        self.filenames = self.df['ImageId'].values\n",
    "        self.len = len(self.filenames)\n",
    "        \n",
    "    # You must override __getitem__ and __len__\n",
    "    def get_mask(self, ImageId):\n",
    "        img_masks = self.masks.loc[self.masks['ImageId'] == ImageId, 'EncodedPixels'].tolist()\n",
    "\n",
    "        # Take the individual ship masks and create a single mask array for all ships\n",
    "        all_masks = np.zeros((768, 768))\n",
    "        if img_masks == [-1]:\n",
    "            return all_masks\n",
    "        for mask in img_masks:\n",
    "            all_masks += rle_decode(mask)\n",
    "        return all_masks\n",
    "    \n",
    "    def get_label(self, ImageId):\n",
    "        '''Returns a label: 0 - no ship, 1 - has one or more ships.'''\n",
    "        label = int(self.df[self.df['ImageId']==ImageId]['has_ship'].values[0])\n",
    "        return label\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"       \n",
    "        \n",
    "        image = Image.open(str(self.path_train + self.filenames[index]))\n",
    "        ImageId = self.filenames[index]\n",
    "        label = self.get_label(ImageId)\n",
    "        mask = self.get_mask(ImageId)            \n",
    "        if self.aug:\n",
    "            data = {\"image\": np.array(image), \"mask\": mask}\n",
    "            transformed = self.transform(**data)\n",
    "            image = transformed['image']/255\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            return image, transformed['mask'][np.newaxis,:,:], label\n",
    "        else:\n",
    "        \n",
    "            return self.transform(image), mask[np.newaxis,:,:], label \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Total number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "class AirbusDS_val(AirbusDS_train):\n",
    "    \"\"\"\n",
    "    A customized data loader.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset\n",
    "        \"\"\"       \n",
    "        \n",
    "        image = Image.open(str(self.path_train + self.filenames[index]))\n",
    "        ImageId = self.filenames[index]\n",
    "        label = self.get_label(ImageId)\n",
    "                    \n",
    "        if self.aug:\n",
    "            data = {\"image\": np.array(image)}\n",
    "            transformed = self.transform(**data)\n",
    "            image = transformed['image']/255\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            return image, label\n",
    "        else:\n",
    "            return self.transform(image), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AirbusDS:\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "        args (dict): Dictionary of (command line) arguments.\n",
    "            Needs to contain batch_size (int) and workers(int).\n",
    "        is_gpu (bool): True if CUDA is enabled.\n",
    "            Sets value of pin_memory in DataLoader.\n",
    "\n",
    "    Attributes:\n",
    "        trainset (torch.utils.data.TensorDataset): Training set wrapper.\n",
    "        valset (torch.utils.data.TensorDataset): Validation set wrapper.\n",
    "        train_loader (torch.utils.data.DataLoader): Training set loader with shuffling.\n",
    "        val_loader (torch.utils.data.DataLoader): Validation set loader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_gpu, batch_size, workers, root, \\\n",
    "                 aug=False, resize_factor=1, empty_frac=0.33, test_size=0.1):\n",
    "        \n",
    "        self.root = root\n",
    "        self.path_train = root + 'train/'\n",
    "        self.aug = aug\n",
    "        self.empty_frac = empty_frac\n",
    "        self.resize_factor = resize_factor\n",
    "        self.test_size = test_size\n",
    "        \n",
    "        exclude_list = ['6384c3e78.jpg','13703f040.jpg', '14715c06d.jpg',  '33e0ff2d5.jpg',\n",
    "                '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg', \n",
    "                'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n",
    "                'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg'] # corrupted images   \n",
    "    \n",
    "        # Calculate masks\n",
    "        masks = pd.read_csv(str(self.root+'train_ship_segmentations.csv')).fillna(-1)\n",
    "        masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\n",
    "        \n",
    "        # Calculate the number of ships on the images\n",
    "        unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "        unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "        unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "        \n",
    "        # Drop corrupted images\n",
    "        unique_img_ids = unique_img_ids[~unique_img_ids['ImageId'].isin(exclude_list)]\n",
    "        self.images_df = unique_img_ids\n",
    "        \n",
    "        masks.drop(['ships'], axis=1, inplace=True)\n",
    "        self.masks = masks \n",
    "        \n",
    "        ##########################\n",
    "\n",
    "        self.trainset, self.valset = self.get_dataset()\n",
    "\n",
    "        self.train_loader, self.val_loader = self.get_dataset_loader (batch_size, workers, is_gpu)\n",
    "\n",
    "        self.val_loader.dataset.class_to_idx = {'no_ship': 0, 'ship': 1}\n",
    "\n",
    "\n",
    "    def get_dataset(self):\n",
    "        \"\"\"\n",
    "        Loads and wraps training and validation datasets\n",
    "\n",
    "        Returns:\n",
    "             torch.utils.data.TensorDataset: trainset, valset\n",
    "        \"\"\"\n",
    "        \n",
    "        # Split dataset to train and validate sets to evaluate the model\n",
    "        train_ids, val_ids = train_test_split(self.images_df, test_size=test_size)\n",
    "        self.val_ids = val_ids\n",
    "        \n",
    "        # Drop small images (mostly just clouds, water or corrupted)\n",
    "        train_ids['file_size_kb'] = train_ids['ImageId'].map(\\\n",
    "                            lambda c_img_id: os.stat(os.path.join(self.path_train, c_img_id)).st_size/1024)\n",
    "        train_ids = train_ids[train_ids['file_size_kb']>40] # keep only >40kb files\n",
    "        \n",
    "        # Undersample empty images to balance the dataset\n",
    "        ships = train_ids[train_ids['has_ship']==1] \n",
    "        no_ships = train_ids[train_ids['has_ship']==0].sample(frac=self.empty_frac)  # Take only this fraction of empty images\n",
    "        self.train_ids = pd.concat([ships, no_ships], axis=0)        \n",
    "               \n",
    "        \n",
    "        # Define transformations for augmentation and without it\n",
    "        self.transform_no_aug = transforms.Compose([transforms.Resize((int(768/resize_factor), int(768/resize_factor))),\n",
    "                                                 transforms.ToTensor()])\n",
    "        if self.aug:\n",
    "            self.transform = Compose([Resize(height=int(768/resize_factor), width=int(768/resize_factor)),\n",
    "                                      OneOf([RandomRotate90(), Transpose(), Flip()], p=0.3)])\n",
    "        else:\n",
    "            self.transform = self.transform_no_aug\n",
    "\n",
    "        # TensorDataset wrapper\n",
    "        trainset = AirbusDS_train(self.path_train, self.aug, self.transform, self.train_ids, self.masks)\n",
    "        valset = AirbusDS_val(self.path_train, False, self.transform_no_aug, self.val_ids, self.masks) \n",
    "\n",
    "        return trainset, valset\n",
    "\n",
    "    def get_dataset_loader(self, batch_size, workers, is_gpu):\n",
    "        \"\"\"\n",
    "        Defines the dataset loader for wrapped dataset\n",
    "\n",
    "        Parameters:\n",
    "            batch_size (int): Defines the batch size in data loader\n",
    "            workers (int): Number of parallel threads to be used by data loader\n",
    "            is_gpu (bool): True if CUDA is enabled so pin_memory is set to True\n",
    "\n",
    "        Returns:\n",
    "             torch.utils.data.TensorDataset: trainset, valset\n",
    "        \"\"\"\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=True,\n",
    "                                                   num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
    "        test_loader = torch.utils.data.DataLoader(self.valset, batch_size=batch_size, shuffle=True,\n",
    "                                                  num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
    "\n",
    "        return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "is_gpu = torch.cuda.is_available()\n",
    "batch_size = 4\n",
    "workers = 4\n",
    "path = '../airbus/'\n",
    "aug=True\n",
    "resize_factor=4\n",
    "empty_frac=0.3\n",
    "test_size=0.1\n",
    "    \n",
    "dataset = AirbusDS(torch.cuda.is_available(), batch_size, workers, \\\n",
    "                   path, aug, resize_factor, empty_frac, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16*45*45, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16*45*45)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_fashion(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_fashion, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, 5) # input features, output features, kernel size\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.mp1 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) # input features, output features, kernel size\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.mp2 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
    "        \n",
    "        self.fc = nn.Linear(64*45*45, num_classes) # 4x4 is the remaining spatial resolution here\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mp1(self.act1(self.conv1(x)))\n",
    "        x = self.mp2(self.act2(self.conv2(x)))\n",
    "        # The view flattens the output to a vector (the representation needed by the classifier)\n",
    "        x = x.view(-1, 64*45*45)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining optimization criterion and optimizer\n",
    "A good baseline is a Cross Entropy loss (Log Softmax + negative log-likelihood) and a stochastic gradient descent (SGD) algorithm with a baseline learning rate of 0.01. If we want to we can use additional momenta or regularization terms (such as L2 - Tikhonov regularization commonly reffered to as weight-decay in ML). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function (criterion)\n",
    "model = CNN_fashion(2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# we can use advanced stochastic gradient descent algorithms \n",
    "# with regularization (weight-decay) or momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring and calculating accuracy\n",
    "We add a convenience class to keep track and average concepts such as processing or data loading speeds, losses and accuracies. For this we need to define a function to define accuracy, which could be based on the absolute accuracy, or top-1 accuracy. Often times in Machine Learning other metrics are employed. For example, in the ImageNet ILSVRC challenge with a classification problem containing a 1000 classes, it is common to report the top-5 accuracy. Here a prediction is counted as accurate if the correct class lies within the top-5 most likely output classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(outputs, targets):\n",
    "    \"\"\"\n",
    "    Evaluates a model's top k accuracy\n",
    "\n",
    "    Parameters:\n",
    "        outputs (torch.autograd.Variable): model output\n",
    "        targets (torch.autograd.Variable): ground-truths/labels\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        float: percentage of correct predictions\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    _, pred = torch.max(outputs.data, 1)\n",
    "    correct = (pred == targets).sum().item()\n",
    "\n",
    "    res = 100 * correct / batch_size\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function (sometimes referred to as \"hook\")\n",
    "The training function needs to loop through the entire dataset in steps of mini-batches (for SGD). For each mini-batch the output of the model and losses are calculated and a \"backward\" pass is done in order to do an update to the model's weights. When the entire dataset has been processed once, one epoch of the training has been conducted. It is common to shuffle the dataset after each epoch. In this implementation this is handled by the \"sampler\" of the dataset loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Trains/updates the model for one epoch on the training dataset.\n",
    "\n",
    "    Parameters:\n",
    "        train_loader (torch.utils.data.DataLoader): The trainset dataloader\n",
    "        model (torch.nn.module): Model to be trained\n",
    "        criterion (torch.nn.criterion): Loss function\n",
    "        optimizer (torch.optim.optimizer): optimizer instance like SGD or Adam\n",
    "        device (string): cuda or cpu\n",
    "    \"\"\"\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, masks, targets = data\n",
    "        inputs = inputs.to(device).float()\n",
    "        targets = targets.to(device).long()\n",
    "        \n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        #prec1 = accuracy(outputs, targets)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        #top1.update(prec1, inputs.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print('Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(loss=losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation function\n",
    "Validation is similar to the training loop, but on a separate dataset with the exception that no update to the weights is performed. This way we can monitor the generalization ability of our model and check whether it is overfitting (memorizing) the training dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, model, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates/validates the model\n",
    "\n",
    "    Parameters:\n",
    "        val_loader (torch.utils.data.DataLoader): The validation or testset dataloader\n",
    "        model (torch.nn.module): Model to be evaluated/validated\n",
    "        criterion (torch.nn.criterion): Loss function\n",
    "        device (string): cuda or cpu\n",
    "    \"\"\"\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    confusion = meter.ConfusionMeter(len(val_loader.dataset.class_to_idx), normalized=True)\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device).float()\n",
    "        targets = targets.to(device).long()\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(outputs, targets)\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        top1.update(prec1, inputs.size(0))\n",
    "\n",
    "        # add to confusion matrix\n",
    "        confusion.add(outputs.data, targets)\n",
    "\n",
    "    print(' * Validation accuracy: Prec@1 {top1.avg:.3f} '.format(top1=top1))\n",
    "    print('Confusion matrix: ', confusion.value())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "TRAIN\n",
      "Loss 0.0813 (0.0813)\t\n",
      "Loss 0.2086 (0.5266)\t\n",
      "Loss 0.2873 (0.5148)\t\n",
      "Loss 0.4216 (0.5026)\t\n",
      "Loss 0.4520 (0.5017)\t\n",
      "Loss 0.7439 (0.5013)\t\n",
      "Loss 0.2928 (0.5003)\t\n",
      "Loss 0.2476 (0.5009)\t\n",
      "Loss 0.1594 (0.4987)\t\n",
      "Loss 0.4901 (0.4986)\t\n",
      "Loss 0.4262 (0.4962)\t\n",
      "Loss 0.6380 (0.4966)\t\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 82.262 \n",
      "Confusion matrix:  [[7086  477]\n",
      " [1369 1475]]\n",
      "Confusion matrix (norm.):  [[2362.          159.        ]\n",
      " [ 456.33333333  491.66666667]]\n",
      "EPOCH: 2\n",
      "TRAIN\n",
      "Loss 0.5394 (0.5394)\t\n",
      "Loss 0.2643 (0.4962)\t\n",
      "Loss 0.3627 (0.4932)\t\n",
      "Loss 0.6023 (0.4948)\t\n",
      "Loss 0.7958 (0.4910)\t\n",
      "Loss 0.3527 (0.4886)\t\n",
      "Loss 1.0726 (0.4873)\t\n",
      "Loss 0.3250 (0.4910)\t\n",
      "Loss 0.2918 (0.4905)\t\n",
      "Loss 0.9821 (0.4918)\t\n",
      "Loss 0.3373 (0.4920)\t\n",
      "Loss 0.3173 (0.4929)\t\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 80.571 \n",
      "Confusion matrix:  [[7131  432]\n",
      " [1590 1254]]\n",
      "Confusion matrix (norm.):  [[2377.  144.]\n",
      " [ 530.  418.]]\n",
      "EPOCH: 3\n",
      "TRAIN\n",
      "Loss 0.8006 (0.8006)\t\n",
      "Loss 0.5684 (0.5173)\t\n",
      "Loss 0.4313 (0.5463)\t\n",
      "Loss 0.7568 (0.5286)\t\n",
      "Loss 0.2773 (0.5222)\t\n",
      "Loss 0.1613 (0.5170)\t\n",
      "Loss 0.5738 (0.5153)\t\n",
      "Loss 0.4553 (0.5103)\t\n",
      "Loss 0.3091 (0.5050)\t\n",
      "Loss 0.2374 (0.5049)\t\n",
      "Loss 0.2640 (0.5054)\t\n",
      "Loss 0.9119 (0.5033)\t\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 81.791 \n",
      "Confusion matrix:  [[6372 1191]\n",
      " [ 704 2140]]\n",
      "Confusion matrix (norm.):  [[2124.          397.        ]\n",
      " [ 234.66666667  713.33333333]]\n",
      "EPOCH: 4\n",
      "TRAIN\n",
      "Loss 0.2680 (0.2680)\t\n",
      "Loss 0.2503 (0.4791)\t\n",
      "Loss 0.1543 (0.4893)\t\n",
      "Loss 0.2538 (0.4876)\t\n",
      "Loss 0.2547 (0.4840)\t\n",
      "Loss 1.6359 (0.4841)\t\n",
      "Loss 0.2435 (0.4853)\t\n",
      "Loss 0.5518 (0.4865)\t\n",
      "Loss 0.6336 (0.4876)\t\n",
      "Loss 0.2395 (0.4881)\t\n",
      "Loss 0.2657 (0.4880)\t\n",
      "Loss 0.3359 (0.4854)\t\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 82.464 \n",
      "Confusion matrix:  [[6523 1040]\n",
      " [ 785 2059]]\n",
      "Confusion matrix (norm.):  [[2174.33333333  346.66666667]\n",
      " [ 261.66666667  686.33333333]]\n",
      "EPOCH: 5\n",
      "TRAIN\n",
      "Loss 0.2860 (0.2860)\t\n",
      "Loss 0.6768 (0.4783)\t\n",
      "Loss 0.6588 (0.4908)\t\n",
      "Loss 1.1566 (0.4916)\t\n",
      "Loss 0.3178 (0.4904)\t\n",
      "Loss 0.3002 (0.4877)\t\n",
      "Loss 0.4779 (0.4857)\t\n",
      "Loss 0.4702 (0.4852)\t\n",
      "Loss 0.3122 (0.4842)\t\n",
      "Loss 0.9269 (0.4838)\t\n",
      "Loss 1.9856 (0.4849)\t\n",
      "Loss 0.4847 (0.4860)\t\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 78.889 \n",
      "Confusion matrix:  [[5920 1643]\n",
      " [ 554 2290]]\n",
      "Confusion matrix (norm.):  [[1973.33333333  547.66666667]\n",
      " [ 184.66666667  763.33333333]]\n",
      "EPOCH: 6\n",
      "TRAIN\n",
      "Loss 0.5624 (0.5624)\t\n",
      "Loss 0.7146 (0.5123)\t\n",
      "Loss 0.4446 (0.4954)\t\n",
      "Loss 0.2029 (0.4895)\t\n",
      "Loss 0.7464 (0.4869)\t\n",
      "Loss 0.6088 (0.5000)\t\n",
      "Loss 0.6464 (0.5008)\t\n",
      "Loss 0.0894 (0.4941)\t\n",
      "Loss 0.3374 (0.4934)\t\n",
      "Loss 0.4727 (0.4926)\t\n",
      "Loss 0.3936 (0.4906)\t\n",
      "Loss 1.0437 (0.4891)\t\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 82.137 \n",
      "Confusion matrix:  [[6542 1021]\n",
      " [ 838 2006]]\n",
      "Confusion matrix (norm.):  [[2180.66666667  340.33333333]\n",
      " [ 279.33333333  668.66666667]]\n",
      "EPOCH: 7\n",
      "TRAIN\n",
      "Loss 0.7408 (0.7408)\t\n",
      "Loss 0.3329 (0.4935)\t\n",
      "Loss 0.6797 (0.4821)\t\n",
      "Loss 0.0672 (0.4807)\t\n",
      "Loss 0.2035 (0.4767)\t\n",
      "Loss 0.5874 (0.4788)\t\n",
      "Loss 0.2386 (0.4781)\t\n",
      "Loss 0.2478 (0.4789)\t\n",
      "Loss 0.7237 (0.4776)\t\n",
      "Loss 0.3141 (0.4787)\t\n",
      "Loss 0.4176 (0.4809)\t\n",
      "Loss 0.6025 (0.4814)\t\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 82.070 \n",
      "Confusion matrix:  [[6489 1074]\n",
      " [ 792 2052]]\n",
      "Confusion matrix (norm.):  [[2163.  358.]\n",
      " [ 264.  684.]]\n",
      "EPOCH: 8\n",
      "TRAIN\n",
      "Loss 0.6569 (0.6569)\t\n",
      "Loss 0.1053 (0.4705)\t\n",
      "Loss 0.4918 (0.4797)\t\n",
      "Loss 0.5044 (0.4805)\t\n",
      "Loss 0.5632 (0.4756)\t\n",
      "Loss 0.4330 (0.4737)\t\n",
      "Loss 0.1237 (0.4727)\t\n",
      "Loss 0.5322 (0.4724)\t\n",
      "Loss 0.5605 (0.4753)\t\n",
      "Loss 0.5415 (0.4747)\t\n",
      "Loss 0.5421 (0.4728)\t\n",
      "Loss 0.6760 (0.4739)\t\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 75.699 \n",
      "Confusion matrix:  [[5441 2122]\n",
      " [ 407 2437]]\n",
      "Confusion matrix (norm.):  [[1813.66666667  707.33333333]\n",
      " [ 135.66666667  812.33333333]]\n",
      "EPOCH: 9\n",
      "TRAIN\n",
      "Loss 0.4940 (0.4940)\t\n",
      "Loss 0.6957 (0.4538)\t\n",
      "Loss 0.7262 (0.4687)\t\n",
      "Loss 0.6357 (0.4696)\t\n",
      "Loss 0.7503 (0.4712)\t\n",
      "Loss 0.6408 (0.4722)\t\n",
      "Loss 0.3652 (0.4741)\t\n",
      "Loss 0.5182 (0.4711)\t\n",
      "Loss 0.4279 (0.4708)\t\n",
      "Loss 0.7003 (0.4694)\t\n",
      "Loss 0.2445 (0.4717)\t\n",
      "Loss 0.5628 (0.4726)\t\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 81.599 \n",
      "Confusion matrix:  [[6675  888]\n",
      " [1027 1817]]\n",
      "Confusion matrix (norm.):  [[2225.          296.        ]\n",
      " [ 342.33333333  605.66666667]]\n",
      "EPOCH: 10\n",
      "TRAIN\n",
      "Loss 0.5145 (0.5145)\t\n",
      "Loss 0.2020 (0.4698)\t\n",
      "Loss 0.9698 (0.4789)\t\n",
      "Loss 0.3453 (0.4775)\t\n",
      "Loss 0.0795 (0.4840)\t\n",
      "Loss 0.3512 (0.4861)\t\n",
      "Loss 0.2408 (0.4865)\t\n",
      "Loss 0.2520 (0.4871)\t\n",
      "Loss 0.1448 (0.4842)\t\n",
      "Loss 0.8649 (0.4845)\t\n",
      "Loss 0.1986 (0.4826)\t\n",
      "Loss 0.4321 (0.4801)\t\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 78.255 \n",
      "Confusion matrix:  [[5815 1748]\n",
      " [ 515 2329]]\n",
      "Confusion matrix (norm.):  [[1938.33333333  582.66666667]\n",
      " [ 171.66666667  776.33333333]]\n"
     ]
    }
   ],
   "source": [
    "total_epochs = 10\n",
    "for epoch in range(total_epochs):\n",
    "    print(\"EPOCH:\", epoch + 1)\n",
    "    print(\"TRAIN\")\n",
    "    train(dataset.train_loader, model, criterion, optimizer, device)\n",
    "    print(\"VALIDATION\")\n",
    "    validate(dataset.val_loader, model, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "confus = np.array([[5815, 1748],[ 515, 2329]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.55875853, 0.04948592],\n",
       "       [0.16796387, 0.22379168]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confus.T/len(dataset.valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type CNN_fashion. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "torch.save(model, '../CNN_fashion.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
