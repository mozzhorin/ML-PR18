{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks for Fashion MNIST in PyTorch\n",
    "We will extend our previous MLP from scratch example by re-implementing the same content in PyTorch. This may seem like a tour-de-force, but will show just exactly how much of the complicated underlying implementation is abstracted away from the user in modern Deep Learning frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dataset class extended to use directly in PyTorch\n",
    "We can basically take our given dataset loader and use it almost as is.\n",
    "There is one modification that we absolutely have to make which is converting the numpy arrays to torch tensors.\n",
    "The function \"torch.from_numpy()\" can be used for this purpose. \n",
    "\n",
    "Two additional features we can add is the use of PyTorch dataset and dataloader structures that are very convenient to use and highly efficient. \n",
    "These are called \"torch.utils.data.TensorDataset\" and \"torch.utils.data.DataLoader\" and allow for the use of a multi-threaded dataset loader.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "import struct\n",
    "import gzip\n",
    "import errno\n",
    "import numpy as np\n",
    "\n",
    "class FashionMNIST:\n",
    "    \"\"\"\n",
    "    Fashion MNIST dataset featuring gray-scale 28x28 images of\n",
    "    fashion items belonging to ten different classes.\n",
    "    Dataloader adapted from MNIST.\n",
    "    We do not define __getitem__ and __len__ in this class\n",
    "    as we are using torch.utils.data.TensorDataSet which\n",
    "    already implements these methods.\n",
    "\n",
    "    Parameters:\n",
    "        args (dict): Dictionary of (command line) arguments.\n",
    "            Needs to contain batch_size (int) and workers(int).\n",
    "        is_gpu (bool): True if CUDA is enabled.\n",
    "            Sets value of pin_memory in DataLoader.\n",
    "\n",
    "    Attributes:\n",
    "        trainset (torch.utils.data.TensorDataset): Training set wrapper.\n",
    "        valset (torch.utils.data.TensorDataset): Validation set wrapper.\n",
    "        train_loader (torch.utils.data.DataLoader): Training set loader with shuffling.\n",
    "        val_loader (torch.utils.data.DataLoader): Validation set loader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_gpu, batch_size, workers):\n",
    "        self.path = os.path.expanduser('datasets/FashionMNIST')\n",
    "        self.__download()\n",
    "\n",
    "        self.trainset, self.valset = self.get_dataset()\n",
    "\n",
    "        self.train_loader, self.val_loader = self.get_dataset_loader(batch_size, workers, is_gpu)\n",
    "\n",
    "        self.val_loader.dataset.class_to_idx = {'T-shirt/top': 0,\n",
    "                                                'Trouser': 1,\n",
    "                                                'Pullover': 2,\n",
    "                                                'Dress': 3,\n",
    "                                                'Coat': 4,\n",
    "                                                'Sandal': 5,\n",
    "                                                'Shirt': 6,\n",
    "                                                'Sneaker': 7,\n",
    "                                                'Bag': 8,\n",
    "                                                'Ankle boot': 9}\n",
    "\n",
    "    def __check_exists(self):\n",
    "        \"\"\"\n",
    "        Checks if dataset has already been downloaded\n",
    "\n",
    "        Returns:\n",
    "             bool: True if downloaded dataset has been found\n",
    "        \"\"\"\n",
    "\n",
    "        return os.path.exists(os.path.join(self.path, 'train-images-idx3-ubyte.gz')) and \\\n",
    "               os.path.exists(os.path.join(self.path, 'train-labels-idx1-ubyte.gz')) and \\\n",
    "               os.path.exists(os.path.join(self.path, 't10k-images-idx3-ubyte.gz')) and \\\n",
    "               os.path.exists(os.path.join(self.path, 't10k-labels-idx1-ubyte.gz'))\n",
    "\n",
    "    def __download(self):\n",
    "        \"\"\"\n",
    "        Downloads the Fashion-MNIST dataset from the web if dataset\n",
    "        hasn't already been downloaded.\n",
    "        \"\"\"\n",
    "\n",
    "        from six.moves import urllib\n",
    "\n",
    "        if self.__check_exists():\n",
    "            return\n",
    "\n",
    "        print(\"Downloading FashionMNIST dataset\")\n",
    "        urls = [\n",
    "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/train-images-idx3-ubyte.gz',\n",
    "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/train-labels-idx1-ubyte.gz',\n",
    "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/t10k-images-idx3-ubyte.gz',\n",
    "            'https://cdn.rawgit.com/zalandoresearch/fashion-mnist/ed8e4f3b/data/fashion/t10k-labels-idx1-ubyte.gz',\n",
    "        ]\n",
    "\n",
    "        # download files\n",
    "        try:\n",
    "            os.makedirs(self.path)\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        for url in urls:\n",
    "            print('Downloading ' + url)\n",
    "            data = urllib.request.urlopen(url)\n",
    "            filename = url.rpartition('/')[2]\n",
    "            file_path = os.path.join(self.path, filename)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(data.read())\n",
    "\n",
    "        print('Done!')\n",
    "\n",
    "    def __get_fashion_mnist(self, path, kind='train'):\n",
    "        \"\"\"\n",
    "        Load Fashion-MNIST data\n",
    "\n",
    "        Parameters:\n",
    "            path (str): Base directory path containing .gz files for\n",
    "                the Fashion-MNIST dataset\n",
    "            kind (str): Accepted types are 'train' and 't10k' for\n",
    "                training and validation set stored in .gz files\n",
    "\n",
    "        Returns:\n",
    "            numpy.array: images, labels\n",
    "        \"\"\"\n",
    "\n",
    "        labels_path = os.path.join(path,\n",
    "                                   '%s-labels-idx1-ubyte.gz'\n",
    "                                   % kind)\n",
    "        images_path = os.path.join(path,\n",
    "                                   '%s-images-idx3-ubyte.gz'\n",
    "                                   % kind)\n",
    "\n",
    "        with gzip.open(labels_path, 'rb') as lbpath:\n",
    "            struct.unpack('>II', lbpath.read(8))\n",
    "            labels = np.frombuffer(lbpath.read(), dtype=np.uint8)\n",
    "\n",
    "        with gzip.open(images_path, 'rb') as imgpath:\n",
    "            struct.unpack(\">IIII\", imgpath.read(16))\n",
    "            images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "        return images, labels\n",
    "\n",
    "    def get_dataset(self):\n",
    "        \"\"\"\n",
    "        Loads and wraps training and validation datasets\n",
    "\n",
    "        Returns:\n",
    "             torch.utils.data.TensorDataset: trainset, valset\n",
    "        \"\"\"\n",
    "\n",
    "        x_train, y_train = self.__get_fashion_mnist(self.path, kind='train')\n",
    "        x_val, y_val = self.__get_fashion_mnist(self.path, kind='t10k')\n",
    "\n",
    "        # convert to torch tensors in range [0, 1]\n",
    "        x_train = torch.from_numpy(x_train).float() / 255\n",
    "        y_train = torch.from_numpy(y_train).int()\n",
    "        x_val = torch.from_numpy(x_val).float() / 255\n",
    "        y_val = torch.from_numpy(y_val).int()\n",
    "\n",
    "        # resize flattened array of images for input to a CNN\n",
    "        x_train.resize_(x_train.size(0), 1, 28, 28)\n",
    "        x_val.resize_(x_val.size(0), 1, 28, 28)\n",
    "\n",
    "        # TensorDataset wrapper\n",
    "        trainset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "        valset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "\n",
    "        return trainset, valset\n",
    "\n",
    "    def get_dataset_loader(self, batch_size, workers, is_gpu):\n",
    "        \"\"\"\n",
    "        Defines the dataset loader for wrapped dataset\n",
    "\n",
    "        Parameters:\n",
    "            batch_size (int): Defines the batch size in data loader\n",
    "            workers (int): Number of parallel threads to be used by data loader\n",
    "            is_gpu (bool): True if CUDA is enabled so pin_memory is set to True\n",
    "\n",
    "        Returns:\n",
    "             torch.utils.data.TensorDataset: trainset, valset\n",
    "        \"\"\"\n",
    "\n",
    "        train_loader = torch.utils.data.DataLoader(self.trainset, batch_size=batch_size, shuffle=True,\n",
    "                                                   num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
    "        test_loader = torch.utils.data.DataLoader(self.valset, batch_size=batch_size, shuffle=True,\n",
    "                                                  num_workers=workers, pin_memory=is_gpu, sampler=None)\n",
    "\n",
    "        return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "is_gpu = torch.cuda.is_available()\n",
    "batch_size = 128\n",
    "workers = 4\n",
    "    \n",
    "dataset = FashionMNIST(torch.cuda.is_available(), batch_size, workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MLP model in PyTorch\n",
    "We now show how to implement a 2 hidden layer MLP in PyTorch. \n",
    "Suitable hidden-layer sizes for this task could be 300 and 100. \n",
    "Depending on the optimization criterion you use, you may want to add something like a Softmax function to your network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, img_size, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(img_size, 300)\n",
    "        self.act1 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(300, 100)\n",
    "        self.act2 = nn.ReLU()\n",
    "        \n",
    "        self.fc3 = nn.Linear(100, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The view flattens the data to a vector (the representation needed by the MLP)\n",
    "        x = x.view(-1, self.img_size)\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.act2(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining optimization criterion and optimizer\n",
    "A good baseline is a Cross Entropy loss (Log Softmax + negative log-likelihood) and a stochastic gradient descent (SGD) algorithm with a baseline learning rate of 0.01. If we want to we can use additional momenta or regularization terms (such as L2 - Tikhonov regularization commonly reffered to as weight-decay in ML). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function (criterion)\n",
    "img_size = 28*28\n",
    "num_classes = 10\n",
    "\n",
    "model = MLP(img_size, num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# we can use advanced stochastic gradient descent algorithms \n",
    "# with regularization (weight-decay) or momentum\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring and calculating accuracy\n",
    "We add a convenience class to keep track and average concepts such as processing or data loading speeds, losses and accuracies. For this we need to define a function to define accuracy, which could be based on the absolute accuracy, or top-1 accuracy. Often times in Machine Learning other metrics are employed. For example, in the ImageNet ILSVRC challenge with a classification problem containing a 1000 classes, it is common to report the top-5 accuracy. Here a prediction is counted as accurate if the correct class lies within the top-5 most likely output classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"\n",
    "    Evaluates a model's top k accuracy\n",
    "\n",
    "    Parameters:\n",
    "        output (torch.autograd.Variable): model output\n",
    "        target (torch.autograd.Variable): ground-truths/labels\n",
    "        topk (list): list of integers specifying top-k precisions\n",
    "            to be computed\n",
    "\n",
    "    Returns:\n",
    "        float: percentage of correct predictions\n",
    "    \"\"\"\n",
    "\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function (sometimes referred to as \"hook\")\n",
    "The training function needs to loop through the entire dataset in steps of mini-batches (for SGD). For each mini-batch the output of the model and losses are calculated and a \"backward\" pass is done in order to do an update to the model's weights. When the entire dataset has been processed once, one epoch of the training has been conducted. It is common to shuffle the dataset after each epoch. In this implementation this is handled by the \"sampler\" of the dataset loader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Trains/updates the model for one epoch on the training dataset.\n",
    "\n",
    "    Parameters:\n",
    "        train_loader (torch.utils.data.DataLoader): The trainset dataloader\n",
    "        model (torch.nn.module): Model to be trained\n",
    "        criterion (torch.nn.criterion): Loss function\n",
    "        optimizer (torch.optim.optimizer): optimizer instance like SGD or Adam\n",
    "        device (string): cuda or cpu\n",
    "    \"\"\"\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for i, (inp, target) in enumerate(train_loader):\n",
    "        inp = inp.to(device)\n",
    "        target = target.to(device).long() # is expected to be int64\n",
    "\n",
    "        # compute output\n",
    "        output = model(inp)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, _ = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), inp.size(0))\n",
    "        top1.update(prec1.item(), inp.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print('Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                   loss=losses, top1=top1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation function\n",
    "Validation is similar to the training loop, but on a separate dataset with the exception that no update to the weights is performed. This way we can monitor the generalization ability of our model and check whether it is overfitting (memorizing) the training dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnet import meter\n",
    "\n",
    "def validate(val_loader, model, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluates/validates the model\n",
    "\n",
    "    Parameters:\n",
    "        val_loader (torch.utils.data.DataLoader): The validation or testset dataloader\n",
    "        model (torch.nn.module): Model to be evaluated/validated\n",
    "        criterion (torch.nn.criterion): Loss function\n",
    "        device (string): cuda or cpu\n",
    "    \"\"\"\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    confusion = meter.ConfusionMeter(len(val_loader.dataset.class_to_idx))\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    for i, (inp, target) in enumerate(val_loader):\n",
    "        inp = inp.to(device)\n",
    "        target = target.to(device).long() # is expected to be int64\n",
    "\n",
    "        # compute output\n",
    "        output = model(inp)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1, _ = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), inp.size(0))\n",
    "        top1.update(prec1.item(), inp.size(0))\n",
    "\n",
    "        # add to confusion matrix\n",
    "        confusion.add(output.data, target)\n",
    "\n",
    "    print(' * Validation accuracy: Prec@1 {top1.avg:.3f} '.format(top1=top1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "TRAIN\n",
      "Loss 2.3082 (2.3082)\tPrec@1 11.719 (11.719)\n",
      "Loss 0.9513 (1.5651)\tPrec@1 66.406 (50.178)\n",
      "Loss 0.7446 (1.1670)\tPrec@1 71.875 (60.281)\n",
      "Loss 0.6037 (0.9909)\tPrec@1 78.125 (65.903)\n",
      "Loss 0.5902 (0.8900)\tPrec@1 81.250 (69.278)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 82.640 \n",
      "EPOCH: 2\n",
      "TRAIN\n",
      "Loss 0.4923 (0.4923)\tPrec@1 80.469 (80.469)\n",
      "Loss 0.5031 (0.5134)\tPrec@1 82.031 (81.327)\n",
      "Loss 0.5391 (0.5043)\tPrec@1 82.031 (81.915)\n",
      "Loss 0.5327 (0.4964)\tPrec@1 79.688 (82.291)\n",
      "Loss 0.3788 (0.4890)\tPrec@1 84.375 (82.688)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 83.580 \n",
      "EPOCH: 3\n",
      "TRAIN\n",
      "Loss 0.4262 (0.4262)\tPrec@1 85.938 (85.938)\n",
      "Loss 0.3988 (0.4469)\tPrec@1 84.375 (84.011)\n",
      "Loss 0.3667 (0.4426)\tPrec@1 86.719 (84.336)\n",
      "Loss 0.3316 (0.4359)\tPrec@1 89.844 (84.632)\n",
      "Loss 0.3408 (0.4383)\tPrec@1 83.594 (84.513)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 85.930 \n",
      "EPOCH: 4\n",
      "TRAIN\n",
      "Loss 0.4299 (0.4299)\tPrec@1 85.156 (85.156)\n",
      "Loss 0.3902 (0.4108)\tPrec@1 85.938 (85.094)\n",
      "Loss 0.4699 (0.4165)\tPrec@1 80.469 (84.911)\n",
      "Loss 0.3615 (0.4084)\tPrec@1 85.938 (85.255)\n",
      "Loss 0.4999 (0.4044)\tPrec@1 82.812 (85.521)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 86.230 \n",
      "EPOCH: 5\n",
      "TRAIN\n",
      "Loss 0.3582 (0.3582)\tPrec@1 90.625 (90.625)\n",
      "Loss 0.4411 (0.3809)\tPrec@1 85.938 (86.355)\n",
      "Loss 0.4050 (0.3810)\tPrec@1 86.719 (86.470)\n",
      "Loss 0.2691 (0.3760)\tPrec@1 92.188 (86.636)\n",
      "Loss 0.4619 (0.3759)\tPrec@1 82.812 (86.559)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 87.270 \n",
      "EPOCH: 6\n",
      "TRAIN\n",
      "Loss 0.2852 (0.2852)\tPrec@1 89.844 (89.844)\n",
      "Loss 0.3086 (0.3703)\tPrec@1 85.938 (86.734)\n",
      "Loss 0.3392 (0.3614)\tPrec@1 88.281 (87.014)\n",
      "Loss 0.3665 (0.3630)\tPrec@1 85.938 (86.999)\n",
      "Loss 0.3983 (0.3630)\tPrec@1 85.938 (86.933)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 87.400 \n",
      "EPOCH: 7\n",
      "TRAIN\n",
      "Loss 0.4398 (0.4398)\tPrec@1 82.812 (82.812)\n",
      "Loss 0.3215 (0.3513)\tPrec@1 87.500 (87.090)\n",
      "Loss 0.4278 (0.3471)\tPrec@1 82.812 (87.395)\n",
      "Loss 0.2139 (0.3451)\tPrec@1 93.750 (87.422)\n",
      "Loss 0.3433 (0.3478)\tPrec@1 89.844 (87.381)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 87.530 \n",
      "EPOCH: 8\n",
      "TRAIN\n",
      "Loss 0.4558 (0.4558)\tPrec@1 82.031 (82.031)\n",
      "Loss 0.3845 (0.3418)\tPrec@1 85.156 (87.500)\n",
      "Loss 0.2601 (0.3325)\tPrec@1 92.188 (87.877)\n",
      "Loss 0.3291 (0.3328)\tPrec@1 87.500 (87.884)\n",
      "Loss 0.2646 (0.3343)\tPrec@1 91.406 (87.892)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 87.900 \n",
      "EPOCH: 9\n",
      "TRAIN\n",
      "Loss 0.3725 (0.3725)\tPrec@1 85.156 (85.156)\n",
      "Loss 0.3707 (0.3239)\tPrec@1 86.719 (87.995)\n",
      "Loss 0.2981 (0.3227)\tPrec@1 89.844 (88.204)\n",
      "Loss 0.3712 (0.3241)\tPrec@1 85.938 (88.138)\n",
      "Loss 0.3064 (0.3243)\tPrec@1 90.625 (88.141)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 88.170 \n",
      "EPOCH: 10\n",
      "TRAIN\n",
      "Loss 0.2981 (0.2981)\tPrec@1 90.625 (90.625)\n",
      "Loss 0.3721 (0.3147)\tPrec@1 85.938 (88.676)\n",
      "Loss 0.3104 (0.3129)\tPrec@1 88.281 (88.717)\n",
      "Loss 0.2507 (0.3134)\tPrec@1 88.281 (88.652)\n",
      "Loss 0.2512 (0.3135)\tPrec@1 90.625 (88.630)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 88.440 \n"
     ]
    }
   ],
   "source": [
    "total_epochs = 10\n",
    "for epoch in range(total_epochs):\n",
    "    print(\"EPOCH:\", epoch + 1)\n",
    "    print(\"TRAIN\")\n",
    "    train(dataset.train_loader, model, criterion, optimizer, device)\n",
    "    print(\"VALIDATION\")\n",
    "    validate(dataset.val_loader, model, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Moving from MLP to CNN\n",
    "Now that we have seen how our two-hidden layer MLP performs, let's see how we can move on to a convolutional neural network (CNN). The advantage of a CNN is that the we no longer have an all-to-all connectivity structure between layers, but rather take a look at local (2-D or even 3-D) neighborhoods. This spatial (or even temporal) filter is then convolved over the whole input (here an image) by \"sharing the weights\" to every position. The outcome is typically referred to as a feature map and in order to check for multiple features we apply a set of such filters in parallel.  \n",
    "\n",
    "Let us see how to build a CNN with 2 layers with a fully-connected classifier on top. You will notice that we have included pooling layers. These layers generally subsample the input and introduce translation invariance (to an extent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, 5) # input features, output features, kernel size\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.mp1 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 5) # input features, output features, kernel size\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.mp2 = nn.MaxPool2d(2, 2) # kernel size, stride\n",
    "        \n",
    "        self.fc = nn.Linear(64*4*4, num_classes) # 4x4 is the remaining spatial resolution here\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mp1(self.act1(self.conv1(x)))\n",
    "        x = self.mp2(self.act2(self.conv2(x)))\n",
    "        # The view flattens the output to a vector (the representation needed by the classifier)\n",
    "        x = x.view(-1, 64*4*4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing and running the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1\n",
      "TRAIN\n",
      "Loss 2.3053 (2.3053)\tPrec@1 10.938 (10.938)\n",
      "Loss 0.7837 (1.2844)\tPrec@1 72.656 (55.097)\n",
      "Loss 0.6906 (0.9735)\tPrec@1 73.438 (65.403)\n",
      "Loss 0.4244 (0.8376)\tPrec@1 85.156 (69.996)\n",
      "Loss 0.5215 (0.7568)\tPrec@1 82.031 (72.874)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 80.360 \n",
      "EPOCH: 2\n",
      "TRAIN\n",
      "Loss 0.6053 (0.6053)\tPrec@1 78.125 (78.125)\n",
      "Loss 0.4939 (0.4694)\tPrec@1 81.250 (83.308)\n",
      "Loss 0.4421 (0.4580)\tPrec@1 84.375 (83.773)\n",
      "Loss 0.4357 (0.4508)\tPrec@1 85.938 (84.043)\n",
      "Loss 0.4957 (0.4406)\tPrec@1 85.156 (84.352)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 86.730 \n",
      "EPOCH: 3\n",
      "TRAIN\n",
      "Loss 0.3465 (0.3465)\tPrec@1 92.188 (92.188)\n",
      "Loss 0.3168 (0.3922)\tPrec@1 91.406 (86.402)\n",
      "Loss 0.5141 (0.3897)\tPrec@1 81.250 (86.408)\n",
      "Loss 0.4393 (0.3870)\tPrec@1 82.031 (86.374)\n",
      "Loss 0.3463 (0.3842)\tPrec@1 87.500 (86.415)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 87.210 \n",
      "EPOCH: 4\n",
      "TRAIN\n",
      "Loss 0.3173 (0.3173)\tPrec@1 89.062 (89.062)\n",
      "Loss 0.3939 (0.3592)\tPrec@1 85.156 (86.966)\n",
      "Loss 0.2729 (0.3546)\tPrec@1 89.844 (87.271)\n",
      "Loss 0.4276 (0.3572)\tPrec@1 82.812 (87.191)\n",
      "Loss 0.3326 (0.3540)\tPrec@1 89.062 (87.375)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 88.440 \n",
      "EPOCH: 5\n",
      "TRAIN\n",
      "Loss 0.3736 (0.3736)\tPrec@1 85.938 (85.938)\n",
      "Loss 0.2913 (0.3242)\tPrec@1 90.625 (88.413)\n",
      "Loss 0.3728 (0.3302)\tPrec@1 86.719 (88.180)\n",
      "Loss 0.2893 (0.3325)\tPrec@1 89.062 (88.092)\n",
      "Loss 0.3251 (0.3295)\tPrec@1 86.719 (88.192)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 89.060 \n",
      "EPOCH: 6\n",
      "TRAIN\n",
      "Loss 0.2508 (0.2508)\tPrec@1 90.625 (90.625)\n",
      "Loss 0.2807 (0.3307)\tPrec@1 88.281 (88.065)\n",
      "Loss 0.3334 (0.3288)\tPrec@1 88.281 (88.227)\n",
      "Loss 0.3428 (0.3208)\tPrec@1 85.156 (88.600)\n",
      "Loss 0.3535 (0.3176)\tPrec@1 86.719 (88.653)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 89.170 \n",
      "EPOCH: 7\n",
      "TRAIN\n",
      "Loss 0.3174 (0.3174)\tPrec@1 87.500 (87.500)\n",
      "Loss 0.3335 (0.3068)\tPrec@1 90.625 (88.954)\n",
      "Loss 0.2827 (0.3014)\tPrec@1 90.625 (89.249)\n",
      "Loss 0.2254 (0.2974)\tPrec@1 93.750 (89.377)\n",
      "Loss 0.3660 (0.2951)\tPrec@1 88.281 (89.448)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 89.420 \n",
      "EPOCH: 8\n",
      "TRAIN\n",
      "Loss 0.3093 (0.3093)\tPrec@1 92.188 (92.188)\n",
      "Loss 0.2446 (0.2829)\tPrec@1 89.844 (89.859)\n",
      "Loss 0.1818 (0.2803)\tPrec@1 93.750 (89.980)\n",
      "Loss 0.2768 (0.2830)\tPrec@1 89.844 (89.854)\n",
      "Loss 0.3322 (0.2835)\tPrec@1 85.938 (89.863)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 90.130 \n",
      "EPOCH: 9\n",
      "TRAIN\n",
      "Loss 0.2660 (0.2660)\tPrec@1 90.625 (90.625)\n",
      "Loss 0.2138 (0.2841)\tPrec@1 93.750 (89.991)\n",
      "Loss 0.3444 (0.2870)\tPrec@1 89.062 (89.918)\n",
      "Loss 0.3370 (0.2789)\tPrec@1 86.719 (90.168)\n",
      "Loss 0.2370 (0.2757)\tPrec@1 92.188 (90.235)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 89.840 \n",
      "EPOCH: 10\n",
      "TRAIN\n",
      "Loss 0.2036 (0.2036)\tPrec@1 93.750 (93.750)\n",
      "Loss 0.3312 (0.2580)\tPrec@1 86.719 (90.826)\n",
      "Loss 0.2245 (0.2648)\tPrec@1 92.188 (90.648)\n",
      "Loss 0.1927 (0.2662)\tPrec@1 94.531 (90.630)\n",
      "Loss 0.2179 (0.2678)\tPrec@1 93.750 (90.557)\n",
      "VALIDATION\n",
      " * Validation accuracy: Prec@1 90.390 \n"
     ]
    }
   ],
   "source": [
    "model = CNN(num_classes).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01,\n",
    "                            momentum=0.9,\n",
    "                            weight_decay=5e-4)\n",
    "\n",
    "total_epochs = 10\n",
    "for epoch in range(total_epochs):\n",
    "    print(\"EPOCH:\", epoch + 1)\n",
    "    print(\"TRAIN\")\n",
    "    train(dataset.train_loader, model, criterion, optimizer, device)\n",
    "    print(\"VALIDATION\")\n",
    "    validate(dataset.val_loader, model, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by changing to a CNN for images we have gained around 2 percent accuracy already. If you want to play around with this example you will be able to gain even more by modifying the network to include regularization methods such as dropout, augmenting or preprocessing your data, constructing larger and deeper models and finding better hyperparameters such as learning rates or mini-batch sizes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well did the model do?\n",
    "In Machine Learning research it is crucial to compare and contrast a model to other researchers implementations. Many of the current Machine Learning datasets are posed as benchmarks where results are rigorously tracked in order to examine the efficiency and efficacy of a model or algorithm proposition.\n",
    "\n",
    "For the fashion MNIST dataset you can check how well both of your models (from scratch and in PyTorch) perform here:\n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/#\n",
    "\n",
    "Do keep in mind that in order to analyze the usefulness of a method one should always compare and contrast on a variety of different datasets with varying task and complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
